{
  "articles": [
    {
      "path": "about.html",
      "title": "Assignment",
      "description": "Assignment submission of course ISSS608 Visual Analytics and Applications - Singapore Management University.",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:14:48+08:00"
    },
    {
      "path": "Data_Preprocessing_and_Exploratory.html",
      "title": "3. Data Preprocessing and Exploratort Data Analysis",
      "description": "In this section, I will apply text mining techniques to perform data preprocessing and conduct exploratory data analysis.\n",
      "author": [
        {
          "name": "Linya Huang",
          "url": {}
        },
        {
          "name": {},
          "url": {}
        }
      ],
      "date": "07-25-2021",
      "contents": "\r\n\r\nContents\r\n3.1 Import packages and social media stream dataImport Packages\r\nImport Dataset\r\n\r\n3.2 Manipulate Datetime\r\n3.3 Text Data Preprocessing\r\n3.4 EDAText Data Word Cloud\r\nMessages from Call Center\r\n\r\n\r\n3.1 Import packages and social media stream data\r\nImport Packages\r\n\r\nShow code\r\npackages= c(\r\n            'tidyverse','data.table','lubridate',\r\n            'textclean','tm','wordcloud','text2vec',\r\n            'topicmodels','tidytext','textmineR','quanteda',\r\n            'BTM','textplot','concaveman','ggwordcloud',\r\n            'qdapDictionaries','textstem','devtools','textnets',\r\n            'igraph', 'tidygraph', \r\n             'ggraph', 'visNetwork','udpipe','grid','sp','sf','gridExtra','mapview')\r\n\r\nfor(p in packages){\r\n  if(!require(p,character.only= T)){\r\n    install.packages(p)\r\n    }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nMain R packages applied:\r\n1) Data Cleaning\r\n‘tidyverse’, ‘lubridate’\r\n2) Text Data Processing\r\n‘textclean’,‘tidytext’,‘BTM’,‘topicmodels’,‘textmineR’,\r\n3) Data Visualization\r\nBasic Charts-\r\n‘tidyverse’\r\nText-\r\n‘textplot’,‘ggwordcloud’,‘wordcloud’,‘textstem’,‘textnets’\r\nNetwork-\r\n‘ggraph’, ‘visNetwork’\r\nMap-\r\n‘sp’,‘sf’,‘gridExtra’,‘mapview’\r\nImport Dataset\r\n\r\nShow code\r\n#read csv file\r\ndata_17_1830=read_csv(\"data/MC3/csv-1700-1830.csv\")\r\ndata_1830_20=read_csv(\"data/MC3/csv-1831-2000.csv\")\r\ndata_20_2130=read_csv(\"data/MC3/csv-2001-2131.csv\")\r\n\r\n#append 3 dataset\r\n\r\ndata=rbindlist(list(data_17_1830,data_1830_20,data_20_2130))\r\n\r\n#print head of data\r\nknitr::kable(head(data,1), \"pipe\")\r\n\r\n\r\ntype\r\ndate(yyyyMMddHHmmss)\r\nauthor\r\nmessage\r\nlatitude\r\nlongitude\r\nlocation\r\nmbdata\r\n2.014012e+13\r\nPOK\r\nFollow us @POK-Kronos\r\nNA\r\nNA\r\nNA\r\n\r\n3.2 Manipulate Datetime\r\nChange datetime string to timestamp by using lubridate package.\r\n\r\nShow code\r\n#timestamp in lubridate\r\ndata$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)\r\ndata$time_1min = cut(data$timestamp, breaks=\"1 min\")\r\ndata$time_30min = cut(data$timestamp, breaks=\"30 min\")\r\n\r\n\r\n\r\n3.3 Text Data Preprocessing\r\nPreprocess text data by lower case transformation, remove punctuation, numbers , stopwords and retrieve RT/ hashtags from message by using regex techniques.\r\n\r\nShow code\r\nppl <- \"@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\"\r\nrt <- \"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.) \"\r\nhash <- \"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\"\r\n\r\n\r\n\r\n\r\ndata$cleaned<-data$message %>% \r\n  str_replace_all(rt,\"\")%>%\r\n  str_replace_all(ppl,\"\")%>%\r\n  str_replace_all(hash,\"\") %>% \r\n  tolower()%>%   # transform all message to lower cases\r\n  replace_contraction()%>%   #replace contractions with long form\r\n  replace_word_elongation()%>% #remove the same letter (case insensitive) appears 3 times consecutively\r\n  str_replace_all(\"[0-9]\", \"\") %>% #removing numbers\r\n  str_replace_all(\"([,=!.?$+%-&])\",\"\")%>% #remove punctuations\r\n  #str_replace_all(\"rt|pokrally|kronosstar\",\"\")%>%\r\n  #|#hi|#pok|#pokrally|\r\n  # #abilapost|#kronosstar|#centralbulletin|@centralbulletin|@kronosstar|rally|aliba\") #remove hashtag and rt\r\n  removeWords(stopwords(\"english\"))%>% \r\n  str_squish()%>% #trim whitespace from a string \r\n  lemmatize_strings()#removes whitespace from start and end of string\r\n\r\n\r\n\r\n# Retrieve RT/ hashtags from message by using regex techniques\r\n\r\nregex <- \"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\"\r\nregex2 <- \"@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\"\r\nregex3 <- \"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.) \"\r\nregex4 <- \"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\"\r\nhash <- \"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\"\r\n\r\ndata$RT_pattern<-str_extract_all(data$message, regex, simplify = TRUE)\r\ndata$RT_from<-  str_extract_all(data$RT_pattern, regex2, simplify = TRUE)\r\ndata$RT_from<-  str_replace(data$RT_from, \"@\",\"\")\r\ndata$RT_message <- str_replace_all(data$message,regex3,\"\")\r\n\r\n\r\ndata$user_mentioned <- str_extract_all(data$message, regex2, simplify = FALSE)\r\n\r\n\r\ndata$user_mentioned<-data$user_mentioned%>% \r\n  str_replace_all(\"character\\\\(0\\\\)|c\\\\(|\\\\)|,\",\"\")\r\n\r\ndata_RT<- data %>% \r\n  filter(RT_from!=\"\")\r\n\r\nknitr::kable(head(subset(data,select=c(\"message\",\"cleaned\"))), \"pipe\")\r\n\r\n\r\nmessage\r\ncleaned\r\nFollow us @POK-Kronos\r\nfollow us\r\nDon’t miss a moment! Follow our live coverage of the POK Rally in the Park!\r\nmiss moment follow live coverage pok rally park\r\nCome join us in the Park! Music tonight at Abila City Park!\r\ncome join us park music tonight abila city park\r\nPOK rally to start in Abila City Park. POK leader Sylvia Marek to open with a speech.<U+0098> #KronosStar\r\npok rally start abila city park pok leader sylvia marek open speech<U+FFFD>\r\nPOK rally set to take place in Abila City Park - POK leader Sylvia Marek has begun with opening remarks #AbilaPost\r\npok rally set take place abila city park - pok leader sylvia marek begin open remark\r\nPOK rally in the park tonight! #POKrally\r\npok rally park tonight\r\n\r\nGet Hashtags with function\r\n\r\nShow code\r\nGetHashtags <- function(tweet) {\r\n\r\n  hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\\\S+', simplify = TRUE) %>% \r\n    as.character()\r\n  \r\n  hashtag.string <- NA\r\n  \r\n  if (length(hashtag.vector) > 0) {\r\n    \r\n    hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')\r\n    \r\n  } \r\n\r\n  return(hashtag.string)\r\n}\r\n\r\nhashtags.df <- tibble(\r\n  Hashtags = data$message %>% map_chr(.f = ~ GetHashtags(tweet = .x))\r\n)\r\nhashtags.df %>% head()\r\n\r\n\r\n# A tibble: 6 x 1\r\n  Hashtags   \r\n  <chr>      \r\n1 <NA>       \r\n2 <NA>       \r\n3 <NA>       \r\n4 #KronosStar\r\n5 #AbilaPost \r\n6 #POKrally  \r\n\r\n3.4 EDA\r\nText Data Word Cloud\r\nUse world cloud to show most frequent words/hastags/tagged users.\r\n\r\nShow code\r\n#convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(data$cleaned)))\r\n#inspect(docs[1:2])\r\n\r\n# Create a document-term-matrix\r\n\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n\r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n\r\n#word cloud\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\nShow code\r\n#wordcloud2(data=df, size=1.6, color='random-dark')\r\n\r\n\r\n#convert dataframe to corpus\r\n\r\ndocs <- Corpus(VectorSource(as.character(hashtags.df$Hashtags)))\r\n#inspect(docs[1:2])\r\n\r\n# Create a document-term-matrix\r\n\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n\r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n\r\n#word cloud\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\nShow code\r\n#convert dataframe to corpus\r\n\r\n\r\ndocs <- Corpus(VectorSource(as.character(data$user_mentioned)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] @POK-Kronos            \r\nShow code\r\n# Create a document-term-matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n\r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n\r\n#word cloud\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\nShow code\r\n#wordcloud2(data=df, size=1.6, color='random-dark')\r\n\r\n\r\n\r\nMessages from Call Center\r\nVisualize count of distinct messages from call center. The most frequent messages from call center are traffic stop, incomplete call for police and alarm-secure no crime. And there are numbers of messages related to suspicious as well.\r\n\r\nShow code\r\ncc_data<-select(data,\"type\", \"message\",\"timestamp\",\"location\")%>% \r\n  filter(type==\"ccdata\")\r\n\r\ncc_data2<-cc_data %>% \r\n  group_by(message) %>%\r\n  count() %>% \r\n  arrange(desc(n))\r\n\r\n\r\nggplot(cc_data2,aes(x=reorder(message,n), y=n)) +\r\n  geom_bar(stat='identity') +\r\n  coord_flip()+\r\n  ggtitle(\"Messages from Call Center\")+\r\n  theme_classic()+\r\n  geom_text(aes(label = n),hjust=1,colour = \"white\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:01+08:00"
    },
    {
      "path": "index.html",
      "title": "Assignment",
      "description": "This is website is for assignment submission of ISSS608 Visual Analytics and Applications.\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:01+08:00"
    },
    {
      "path": "Introduction.html",
      "title": "1. Introduction ",
      "author": [
        {
          "name": "Linya Huang",
          "url": {}
        },
        {
          "name": {},
          "url": {}
        }
      ],
      "date": "07-25-2021",
      "contents": "\r\n\r\nContents\r\n1.1 Overview\r\n1.2 Challenge\r\nSubmission Instructions\r\n\r\n1.1 Overview\r\nThe assignments require students to put the concepts, methods and techniques learned in class to solve real world problem using visual analytics techniques. Students should also use the assignments to gain hands-on experience on using R data visualisation and data analysis packages to complete the assignment.\r\n1.2 Challenge\r\nI will use visual analytics techniques in R to solve questions from VAST Challenge 2021: Mini-Challenge 3 - On January 23, 2014, multiple events unfolded in Abila, the challenge is to perform a retrospective analysis based on limited information about what took place. The goal is to identify risks and how they could have been mitigated more effectively.\r\nThe main data sets includes information of collection of microblogs and emergency calls from the days surrounding the disappearance.\r\nQuestions\r\n1.Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.\r\n2.Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.\r\n3.If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words.\r\n4.If you solved this mini-challenge in 2014, how did you approach it differently this year?(Not Applicable)\r\nSubmission Instructions\r\n• This is an individual assignment. You are required to work on the assignment and prepare submission individually.\r\n• The assignment report must be written by using R Markdown. It can be in either Distill or blogdown format. You are required to publish the assignment report on Netlify and provide the link on the assignment submission page on elearn.\r\n• Upload the assignment report and data onto your individual Github repository and provide the link on the assignment submission page on eLearn.\r\n• Assignment due is on 25th July 2021 (Sunday), mid-night 11:59pm.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:02+08:00"
    },
    {
      "path": "Literature_Review.html",
      "title": "2. Literature Review",
      "description": "Literature review will be conducted on how the analysis were performed before. I will focus on  identifying gaps whereby interactive visual analytics approach can be used to enhance user experience on using the analysis techniques.\n",
      "author": [
        {
          "name": "Linya Huang",
          "url": {}
        },
        {
          "name": {},
          "url": {}
        }
      ],
      "date": "07-25-2021",
      "contents": "\r\n\r\nContents\r\n2.1 Spatiotemporal Abnormal Detection\r\n2.2 Event timeline visualization\r\n2.3 Junk/Spam Detection\r\n2.4 Relationship or text and text entities visualization\r\n2.5 GeoVisual Analysis\r\n2.6 Combination of Geographical and Text Visualization\r\n\r\nDetecting abnormal events ,such as disaster or crisis, from microblog social media has become a trend, as social media has played a pervasive role in the way people behave and think. Nowadays, people are also using time-stamped , geo -located data to share live information about what’s happening in their surroundings, which enables the public, government and researches to sense abnormal events in community more quickly and take immediate actions.\r\n2.1 Spatiotemporal Abnormal Detection\r\nThe study from Junghoon et al. (2014) applied spatiotemporal visualization of microblog data to detect disaster and support decisions. The visualization they applied are shown as below,\r\n The number of Twitter users who posted Twitter messages containing one of the following keywords: hurricane, storm, and sandy is highly related to Hurricane Sandy (as shown in blue pins and blue lines of the center locations)\r\nThen their study also analyzed spatial pattern of Twitter users after the tornado.\r\n (1)the heatmap shows a normal situation of Twitter user distribution in the same area of which the distribution is very different from the situation after the tornado event. As such, the spatial analytics can help with post disaster management according to twitter heatmap distribution, for example, the authority can allocate more resources to the most visited places by the public\r\nBesides, the abnormality score are calculated with Latent Dirichlet Allocation (LDA) topic modeling and Seasonal-Trend Decomposition to detect severe weather condition.\r\nAbnormality of the first topicThe visualization techniques provided very good intuition on visualize location-stamped micro-blog posting to help track disaster status and post disaster management with the different geospatial granularity levels and real-time streaming data states. This is a good inspiration in leveraging on spatiotemporal analysis, LDA modeling and abnormality detection methodology to detect the crisis in the challenge question.\r\n2.2 Event timeline visualization\r\nThe submission by International Institute of Information and Technology has plotted all the sub-events taken and the evidence to describe the timeline of the event.\r\nEvent TimelineThis summary chart helps audience to get a quick and clear overview of major event happened in Abila, which is a good review of challenge question 1. In R package, we can apply timevis pacakge to achieve the function, which enables rich and fully interactive timeline visualizations.\r\n2.3 Junk/Spam Detection\r\nIn Visual Analytics Benchmark Repository 2014, the submission from Tianjin University demonstrated a network approach in identifying junk tweet.\r\n They think “the size of the nodes represents the number of the blog being post. Notice that there are several nodes which have the larger size but no relationship with others which means theirs blogs are ignored by others.”.\r\nThis is a very good intuition to visualize center and edges microblog users. In addition, I think this visualization can be further improved with visnetwork interactivity, that users can click the node and get more information about the nodes (authors,post,number of retweets and so on.)\r\nUniversidad de Buenos Aires analyzed the number of messages by author. With the calculation of spam index, they found that a low spam index value indicates many messages with the same text and thus is associated with a high likelihood of spam.\r\n This is a good inspiration of feature engineering of the given dataset. The visualization can be further improved as the line chart was misused on discrete value and users’ retweet pattern can be added for more insights.\r\n2.4 Relationship or text and text entities visualization\r\nTextnets is a R package for automated text analysis using network techniques.It provides network for both text and text entities, which provides visualization ability to uncover correlation of texts in documents.\r\n { width=70% }\r\nThis visualization can be applied in the question 2 in answering the potential consequences of the situation and the number of people who could be affected via network graph.\r\n2.5 GeoVisual Analysis\r\nSeveral benchmark teams in 2014 had insightful geovisual analytics with microblog text analytics,\r\nFor example, for submission from Peking University, they pointed the location of major events shooting/standoff, fire and hit as well as possible route of black van; for submission from Purdue University, they analyzed the complete trajectory of black van from shooting and hit/run events.\r\n\r\nWith understanding of microblog posting for detailed location, we can review and investigate the kidnap event and find possible locations of suspects. These are good approaches for retrospective investigation. While for real time response, we can try to be more flexible to detect the abnormality in microblogs in time with appropriate text mining techniques.\r\n2.6 Combination of Geographical and Text Visualization\r\nIt is also worth mentioning one of a innovated combination visualization from Purdue University.\r\n As highliged in the example as above, a)temporal pattern of the microblog posting from different sources are tracked properly, b) word cloud of the most frequent words provided information of popular words/topics discussed, C) network graph provides the key opinion leader and its impact to the public, d) and e) detailed microblog posted by ccdata and mbdata, besides, they highlighed the geo location of the events and streets mentioned by call center.\r\nThis visualization is a very good inspiration on how a live tweets can be tracked and visualized with single data set. The analytics has applied various feature engineering techniques for numerical variables, text mining and network relationships. The fruitful and dynamic information from this single combination graph can really help audience to understand the overall situation and make quick decisions or responses to the event.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:03+08:00"
    },
    {
      "path": "Question_Answer.html",
      "title": "Question Answer",
      "description": "Use visual analytics to analyze the available data and evaluate the changing levels of risk to the public and recommend actions.\n",
      "author": [
        {
          "name": "Linya Huang",
          "url": {}
        },
        {
          "name": {},
          "url": {}
        }
      ],
      "date": "07-25-2021",
      "contents": "\r\n\r\nContents\r\n4.Data Visualization for Challenge QuestionsQuestion1Wordcloud trend of 30 minutes time interval\r\nTF_IDF of 30 mintunes time interval\r\nTopic Modeling - Distinguish Junk/Spam/Chatter from Meaningful Events\r\nTopic Trend\r\nTweets/Retweets and Unique Contents\r\n\r\nQuestion 2Number of posts\r\nTweets and Retweets Trend\r\nMonitor important news source - Mainstream media\r\n\r\nQuestion 3Restrospective\r\nReal-Time\r\n\r\n\r\n\r\n\r\nShow code\r\npackages= c('sf','clock','tmap',\r\n            'tidyverse','data.table','lubridate',\r\n            'textclean','tm','wordcloud','text2vec',\r\n            'topicmodels','tidytext','textmineR','quanteda',\r\n            'BTM','textplot','concaveman','ggwordcloud',\r\n            'textstem','devtools','textnets',\r\n            'ggiraph','plotly','igraph', 'tidygraph', \r\n             'ggraph', 'visNetwork','udpipe','grid')\r\n\r\nfor(p in packages){\r\n  if(!require(p,character.only= T)){\r\n    install.packages(p)\r\n    }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n4.Data Visualization for Challenge Questions\r\nQuestion1\r\n4.1 Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.\r\nWordcloud trend of 30 minutes time interval\r\nSplit data by 30 minutes time interval and perform worldcloud in each interval, to visualize the most frequent words in the microblog.\r\n\r\nShow code\r\n#add id to dataset\r\n\r\ndata$id <- seq.int(nrow(data))\r\n\r\ndata_subset=subset(data,select=c(\"time_30min\",\"cleaned\"))\r\n\r\n\r\ndata$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)\r\ndata$time_30min = cut(data$timestamp, breaks=\"30 min\")\r\ndata$id <- seq.int(nrow(data))\r\n\r\ndata_subset=subset(data,select=c(\"time_30min\",\"cleaned\"))\r\n\r\n\r\nusenet_words<-data_subset%>%\r\n  group_by(time_30min) %>% \r\n  unnest_tokens(word, cleaned) %>%\r\n  count(time_30min,word, sort = TRUE)\r\n\r\n\r\nusenet_words[order(usenet_words$time_30min),]\r\n\r\n\r\n# A tibble: 6,975 x 3\r\n# Groups:   time_30min [10]\r\n   time_30min          word       n\r\n   <fct>               <chr>  <int>\r\n 1 2014-01-23 17:00:00 pok       98\r\n 2 2014-01-23 17:00:00 rally     76\r\n 3 2014-01-23 17:00:00 people    44\r\n 4 2014-01-23 17:00:00 park      40\r\n 5 2014-01-23 17:00:00 jakab     37\r\n 6 2014-01-23 17:00:00 lucio     35\r\n 7 2014-01-23 17:00:00 police    35\r\n 8 2014-01-23 17:00:00 can       34\r\n 9 2014-01-23 17:00:00 abila     30\r\n10 2014-01-23 17:00:00 sylvia    30\r\n# ... with 6,965 more rows\r\nShow code\r\nusenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all(\"2014-01-23 \",\"\")\r\n  \r\nl1<-c(\"18:30:00\",\"19:30:00\",\"17:00:00\",\"19:00:00\",\"20:00:00\",\"20:30:00\",\"18:00:00\",\"17:30:00\",\"21:00:00\",\"21:30:00\")\r\nTime_30min<-c(\"18:30-19:00\",\"19:30-20:00\",\"17:00-17:30\",\"19:00-19:30\",\"20:00-20:30\",\"20:30-21:00\",\"18:00-18:30\",\"17:30-18:00\",\"21:00-21:31\",\"21:00-21:31\")\r\ntime_30min_df<-data.frame(l1,Time_30min)\r\nusenet_words<-left_join(usenet_words,time_30min_df,by=c(\"time_30min\"=\"l1\"))\r\n\r\n\r\n\r\nset.seed(1234)\r\nusenet_words %>%\r\n  group_by(Time_30min) %>% \r\n  slice_max(order_by = n, n = 20) %>% \r\nggplot(aes(label = word,\r\n           size = n)) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  facet_wrap(~Time_30min)\r\n\r\n\r\n\r\n\r\nFrom the visualization, we can tell the main content discussing in the community are\r\n1700-1830 pok rally in park with several name mentioned such as cylvia, jakab, viktor;\r\n1830-1930 fire at dancing dolphin partment, police and evacuation;\r\n1930-2000 shooting and cops/police;\r\n2100-2131 van/ hostage/ explosion;\r\nTF_IDF of 30 mintunes time interval\r\nThen, by applying Term Frequency – Inverse document frequency (tf-idf) to bigram texts, we can uncover more information compared with uni-grams.\r\n\r\nShow code\r\ndata_subset$time_30min<-str_replace_all(data_subset$time_30min,\"2014-01-23 21:30:00\",\"2014-01-23 21:00:00\")\r\n\r\nbigrams <- data_subset%>%\r\n  group_by(time_30min)%>%\r\n  unnest_tokens(word,\r\n                cleaned,\r\n                token = \"ngrams\",\r\n                n = 2) %>%\r\n  count(time_30min,word, sort = TRUE) %>% ungroup()\r\n\r\n# bigrams <- data_subset%>%\r\n#   unnest_tokens(word,\r\n#                 cleaned,\r\n#                 token = \"ngrams\",\r\n#                 n = 2) %>%\r\n#   count(time_30min,word, sort = TRUE)\r\n\r\n\r\ntf_idf <- bigrams%>%\r\n  bind_tf_idf(word,time_30min, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\ntf_idf$time_30min<-str_replace_all(tf_idf $time_30min,\"2014-01-23 \",\"\")\r\n\r\nl1<-c(\"18:30:00\",\"19:30:00\",\"17:00:00\",\"19:00:00\",\"20:00:00\",\"20:30:00\",\"18:00:00\",\"17:30:00\",\"21:00:00\",\"21:30:00\")\r\nTime_30min<-c(\"18:30-19:00\",\"19:30-20:00\",\"17:00-17:30\",\"19:00-19:30\",\"20:00-20:30\",\"20:30-21:00\",\"18:00-18:30\",\"17:30-18:00\",\"21:00-21:31\",\"21:00-21:31\")\r\ntime_30min_df<-data.frame(l1,Time_30min)\r\n\r\ntf_idf<-left_join(tf_idf,time_30min_df,by=c(\"time_30min\"=\"l1\"))\r\n\r\n\r\n\r\ntf_idf %>%\r\n  group_by(Time_30min) %>%\r\n  slice_max(tf_idf,\r\n            n = 10) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder(word,\r\n                        tf_idf)) %>%\r\n  ggplot(aes(tf_idf,\r\n             word,\r\n             fill = Time_30min)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ Time_30min,\r\n             scales = \"free\") +\r\n  labs(x = \"TF-IDF Bigram in 30 time interval\",\r\n       y = NULL)\r\n\r\n\r\n\r\n\r\nWithout having to process more on data, the contents of the microblogs with 30 mins time interval is even more informative as tf-idf reflects how important a word is to a document in a collection or corpus.\r\nFor instance, from 1700-1800, we know that police presense at pok rally and Dr. Audrey is mentioned several times as important content. Then from 1830 onwards, fire occurs at dolphin apartment and followed by evacuation. Subsequently, from 1930, a shot/gun fire happened with police involved, followed by injured firefighter and hospitalization. And at 2100 suspects were arrested and then an explosion at dolphin apartment from 2130 onward.\r\nTopic Modeling - Distinguish Junk/Spam/Chatter from Meaningful Events\r\nTo distinguish meaningful events from chatter/junk/spam messages, I will then apply topic modeling techniques to identify topic in each microblogs.\r\nTop five words in each topics\r\n\r\nShow code\r\nwordcorpus <- Corpus(VectorSource(as.character(data$cleaned)))  \r\ndtm <- DocumentTermMatrix(wordcorpus,\r\n                          control = list(\r\n                            wordLengths=c(2, Inf),               # limit word length\r\n                            bounds = list(global = c(5,Inf)),    # minimum word frequency\r\n                            removeNumbers = TRUE,                #remove Numbers\r\n                            weighting = weightTf,                #weighted term frequency\r\n                            encoding = \"UTF-8\"))\r\n\r\nrowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document\r\ndtm.new   <- dtm[rowTotals> 0, ] #remove 0 dtm rows of matrix\r\n\r\ntopic=LDA(dtm.new,k=10,method=\"Gibbs\",conrol=list(seed=2021,alpha=0.01,iter=300))\r\n\r\nterms(topic,5)\r\n\r\n\r\n     Topic 1 Topic 2 Topic 3 Topic 4  Topic 5 Topic 6    Topic 7  \r\n[1,] \"life\"  \"can\"   \"want\"  \"viktor\" \"just\"  \"report\"   \"fire\"   \r\n[2,] \"make\"  \"thing\" \"good\"  \"dr\"     \"come\"  \"cop\"      \"abila\"  \r\n[3,] \"think\" \"look\"  \"time\"  \"newman\" \"run\"   \"van\"      \"dance\"  \r\n[4,] \"live\"  \"one\"   \"stop\"  \"sylvia\" \"like\"  \"shoot\"    \"dolphin\"\r\n[5,] \"one\"   \"great\" \"give\"  \"us\"     \"day\"   \"evacuate\" \"arrive\" \r\n     Topic 8   Topic 9  Topic 10  \r\n[1,] \"get\"     \"police\" \"people\"  \r\n[2,] \"go\"      \"pok\"    \"build\"   \r\n[3,] \"take\"    \"rally\"  \"say\"     \r\n[4,] \"success\" \"park\"   \"resident\"\r\n[5,] \"two\"     \"area\"   \"may\"     \r\n\r\nExtract per-topic-per-word probabilities ,β(“beta”), from the model. The higher the value, the more important of the words’ contribution to the topic.\r\n\r\nShow code\r\nap_topics <- tidy(topic, matrix = \"beta\")\r\n\r\n\r\nap_top_terms <- ap_topics %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n = 10) %>% \r\n  ungroup() %>%\r\n  arrange(topic, -beta)\r\n\r\nap_top_terms %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  scale_y_reordered()\r\n\r\n\r\n\r\n\r\nAs shown above, the meaningful keywords can be found in each topic. From the distribution, we can see that in abnormal topic such as topic 1,5,9 and 10, the top words are dominating the topic, while in other chatter/junk/spam topics, a few normal words has relatively same high beta value. As such, topics can be identified roughly as below.\r\nTopic 1 - police related with evacuation/hostage/standoff\r\nTopic 3 - fire at dancing dolphin\r\nTopic 5- van/guy/cop related\r\nTopic 9 - pok rally\r\nTopic 10 - POK leader\r\nWhile other topics do not have distinguishable words, resulting the contents related to chatter/junk/spam.\r\nTopic Trend\r\nBy using gamma value from topic modeling results, we can then assign each document with a topic.\r\n\r\nShow code\r\ntopic_gamma <- tidy(topic, matrix = \"gamma\")\r\ntopic_gamma <- topic_gamma %>% \r\n  group_by(document) %>% \r\n  slice(which.max(gamma))\r\n\r\ntopic_gamma$document<-as.numeric(topic_gamma$document) \r\n#topic_gamma[order(topic_gamma$document),] %>% group_by(topic) %>% count() \r\n#(topic_gamma%>% arrange(desc(-document)))\r\n\r\nid_time <- data %>% select(c(\"id\",\"time_1min\"))\r\n\r\ntopic_data<-left_join(topic_gamma,id_time,by=c(\"document\"=\"id\"))\r\n\r\n#manually put topics in LDA results\r\n\r\ntopic_c<- c(1,2,3,4,5,6,7,8,9,10)  \r\ntopics_c <- c(\"police\",\"chatter1\",\"chatter2\",\"fire\",\"chatter3\",\r\n\"van\",\"chatter4\",\"chatter5\",\"pokrally\",\"pokleader\")\r\ntopic_df<-data.frame(topic_c,topics_c )\r\n\r\ntopic_data<-left_join(topic_data,topic_df,by=c(\"topic\"=\"topic_c\"))\r\n\r\ntopic_data %>% group_by(time_1min,topics_c) %>% count() %>% \r\n  ggplot(aes(x=time_1min))+\r\n  geom_bar(aes(y=n), stat = \"identity\",fill = \"black\")+\r\n  facet_wrap(~topics_c)+\r\n  theme(axis.title.x=element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.ticks.x=element_blank())+\r\n  ggtitle(\"Topics Trend from 1700-2130\")\r\n\r\n\r\n\r\n\r\nFrom the topic trend distribution, we can see that chatter topic 1-5 are spread all over the time period, and the number posts are small and randomly distributed. While the peaks of key major events are very obvious at certain point of time.\r\nIn summary of topic modeling techniques and visualization, we can identify major event from junk/chatter/spam. The topic trend visualization provide information about temporal pattern of meaningful events distribution and distinguish the meaningful events during the time period.\r\nBiterm Topic Modeling\r\nThe Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns, which is a generative model. In the generation procedure, a biterm is generated by drawing two words independently from a same topic z.\r\n\r\nShow code\r\n#Tokenize data\r\ntidytxtdata<- tidy(dtm)\r\n\r\ntidytxtdata <- tidytxtdata%>% #Remove the count column\r\n  select(-count)\r\ntidytxtdata <- tidytxtdata%>% #Change the column name 'term' to 'word' so that we can get rid of stopwords later\r\n  rename(word = term)\r\n\r\n\r\n#Remove stopwords\r\ntidytxtdata <- tidytxtdata%>%\r\n  anti_join(stop_words)\r\n\r\n\r\n#Use the btm model\r\nset.seed(321)\r\nmodel <- BTM(tidytxtdata, k = 10, beta = 0.01, background = TRUE, iter = 500, trace = 100) #Run the model\r\n\r\ntopicterms <- terms(model, top_n = 10) #View the topics\r\n#topicterms\r\n\r\nlibrary(textplot)\r\nlibrary(ggraph)\r\nlibrary(concaveman)\r\nplot(model)\r\n\r\n\r\n\r\n\r\nBy applying BTM visualization, topic are clusterd in different colors as shown above and words in bold front indicate higher importance in terms of co-occurrence with other words in same topic, which helps to distinguish the meaningful events from typical chatter/junk/spam. For instance, topics in lightblue (with words world-wait-start-improve), lightgreen (with words stop stand) pink (with word mind) have relatively small size of words and tends to be chatter contents. And topic with word life/success in bold face are life related contents which has no meaning. While for topics like fire, van, police, pok rally in bold front are more meaningful and distinguishable contents in the community.\r\nTweets/Retweets and Unique Contents\r\nThen, to derive the feature from tweets and re-tweets cotents. I calculated the unique ratio- number of unique contents divided by total tweets of a single user- and reteweet ratio - number of retweets devided by number of total tweets of a single user. These 2 indexes can help us distinguish influential users and “junk” users.\r\n\r\nShow code\r\ntweet<-data %>%\r\n  filter(author!=\"NA\") %>% \r\n  group_by(author) %>%\r\n  count() %>% \r\n  ungroup()\r\n\r\nretweet<-data %>%\r\n  group_by(RT_from) %>%\r\n  count()%>% \r\n  ungroup()\r\n\r\nrepeat_data<-data %>%\r\n  filter(author!=\"NA\") %>% \r\n  group_by(author,message) %>%\r\n  count() %>% \r\n  ungroup() %>% \r\n  group_by(author) %>%\r\n  count() %>% \r\n  ungroup()\r\n\r\n\r\n\r\ncolnames(tweet)[2]<-\"tweet\"\r\ncolnames(retweet)[2]<-\"retweet\"\r\ncolnames(repeat_data)[2]<-\"repeat\"\r\n\r\ntweet_retweet<-left_join(tweet, retweet, by = c(\"author\"=\"RT_from\"))\r\ntweet_retweet<-left_join(tweet_retweet, repeat_data, by = c(\"author\"=\"author\"))\r\n\r\n\r\n\r\n\r\ntweet_retweet$retweet_ratio<- tweet_retweet$retweet/tweet_retweet$tweet\r\ntweet_retweet$unique_ratio <-tweet_retweet$'repeat'/tweet_retweet$tweet\r\n\r\ntweet_retweet$retweet<-ifelse(is.na(tweet_retweet$retweet),0,tweet_retweet$retweet)\r\ntweet_retweet$retweet_ratio<-ifelse(is.na(tweet_retweet$retweet_ratio),0,tweet_retweet$retweet_ratio)\r\n\r\n\r\ntweet_retweet_2<-tweet_retweet %>% filter(unique_ratio<0.8 | retweet_ratio>=2)\r\n\r\nggplot(data=tweet_retweet_2,aes(x=reorder(author,tweet)))+\r\n  geom_bar(aes(y=tweet),stat = \"identity\",fill=\"blue\",alpha=0.5)+\r\n  geom_bar(aes(y=retweet),stat = \"identity\",fill=\"red\",alpha=0.4)+\r\n  geom_point(mapping=aes( y=unique_ratio*1300), color=\"black\",size=2)+\r\n  scale_y_continuous(limits=c(0, 1300),name = \"Number of Tweets/Re-Tweets\",sec.axis = sec_axis(~ ./1300,name = \"Unique Ratio\"))+\r\n  theme(axis.text.x = element_text(size=8, angle=45),\r\n  panel.border = element_blank(), \r\n  panel.background = element_blank(),\r\n  panel.grid.major = element_blank(),\r\n  panel.grid.minor = element_blank(), \r\n  axis.line = element_line(colour = \"black\"))+\r\n  ggtitle(\"Tweets/Retweets and Unique Tweets Ratio\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n# dt<-subset(data,select=c(timestamp,author,message))\r\n# dt$timestamp<-str_replace_all(dt$timestamp,\"2014-01-23 \",\"\")\r\n# dt$message<-iconv(dt$message, 'utf-8', 'ascii', sub='')\r\n# tweet_retweet_2<-subset(tweet_retweet,select=c(author,tweet,retweet))\r\n# dt<-left_join(dt,tweet_retweet_2,by=c(\"author\"=\"author\"))\r\n# dt<-dt %>% filter(author!=\"\")\r\n# # dt<-dt%>% mutate_if(is.character, ~gsub('[^ -~]', '', .)) # remove characters non UTF-8\r\n# DT::datatable(dt)\r\n\r\n\r\n\r\nThe above chart is plotted by filtering unique_ratio less than 0.8 or retweet_ratio>=2.\r\nUser with number of tweets does not have any re-tweets and the unique contents of their tweets is low, such as @Clevvah4Evah, @KronosQuoth, and they are mainly posting junk/spam blogs.\r\nFor other low unique ratio , users like @grassGreeener,@cheapgoods998,@rockinHW,@junkman377,@carjunkers,@reggierockin776 posting repeated advertisement/spam/junk contents are also “junk” users.\r\nOn the other hand, authors with high re-tweet frequency are @HomelandIlluminations, @AbilaPost, @KronosStar, @CentralBulletin , @NewsOnlineToday , and @InternationalNews, of which the post were more active and meaningful information to the public.\r\nNetwork Visualization in detecting chatter/spam/junk\r\nVisnetwork packages in R provided interactive capability for network visualization.\r\n\r\nShow code\r\nRT_edges_aggregated <-data_RT%>%\r\n  group_by(RT_from,author) %>%\r\n  count() %>%\r\n  ungroup()\r\n\r\nRT_nodes_aggregated <-data%>%\r\n  group_by(author) %>%\r\n  count() %>%\r\n  ungroup\r\n\r\n\r\nRT_nodes_aggregated$id<-seq.int(nrow(RT_nodes_aggregated))\r\nRT_nodes_aggregated<-RT_nodes_aggregated %>% \r\n  rename(label=author,size=n) %>% \r\n  filter(label!=\"\")\r\n#RT_nodes_aggregated$label<-paste(RT_nodes_aggregated$label,RT_nodes_aggregated$size)\r\n\r\n  \r\nRT_edges_aggregated_viz<-\r\n  left_join(RT_edges_aggregated,RT_nodes_aggregated,by=c(\"RT_from\"=\"label\")) %>% \r\n  rename(from=id) %>% \r\n  left_join(RT_nodes_aggregated,by=c(\"author\"=\"label\")) %>% \r\n  rename(to=id)\r\n\r\nRT_edges_aggregated_viz<-subset(RT_edges_aggregated_viz,select=c(\"from\",\"to\"))\r\n\r\n\r\nRT_graph <- tbl_graph(nodes=RT_nodes_aggregated,\r\n                           edges = RT_edges_aggregated,\r\n                           directed = TRUE)\r\n\r\n\r\n\r\n visNetwork(RT_nodes_aggregated, \r\n            RT_edges_aggregated_viz , \r\n            main = \"Retweet Network\",width=\"100%\", height=\"400px\")%>%\r\n   visOptions(highlightNearest = TRUE)%>%\r\n   visNodes(label=\"label\",color = list(background = \"lightblue\", \r\n                        border = \"darkblue\",\r\n                        highlight = \"yellow\"))%>% \r\n  visIgraphLayout(layout = \"layout_with_fr\")\r\n\r\n\r\n\r\n{\"x\":{\"nodes\":{\"label\":[\"aaasTech\",\"AbilaAllFaith\",\"AbilaFireDept\",\"AbilaPoliceDepartment\",\"AbilaPost\",\"aliceRocks\",\"anaregents\",\"blueSunshine\",\"BlueVelvet\",\"BoraVerissimo\",\"brain448\",\"brandonL\",\"brewvebeenserved\",\"brontes_riff\",\"carjunkers\",\"CentralBulletin\",\"cheapgoods998\",\"choconibbs\",\"cleaningFish\",\"Clevvah4Evah\",\"cminvestments11\",\"courage4life\",\"creatorRocks\",\"dangermice\",\"deals4realz\",\"dealsRUs101\",\"dirtdigger334\",\"dolls4sale\",\"dragonRider1\",\"dtennent\",\"dtrejos\",\"eazymoney\",\"electricAvenue\",\"eliza003\",\"ernieO\",\"farmboy\",\"fictionalJoe\",\"footfingers\",\"FriendsOfKronos\",\"gardener4958\",\"grassGreeener\",\"GreyCatCollectibles\",\"grlPwrz505\",\"hazMore445\",\"hempRules\",\"hennyhenhendrix\",\"henri\",\"hermanM\",\"HerraTomas\",\"hngohebo_ABILAPOST\",\"HomelandIlluminations\",\"hotdrugs225\",\"InternationalNews\",\"jaques\",\"jenny90210\",\"jgrobannne\",\"joyBubbles\",\"joyce101\",\"joyousNoise\",\"jsmith\",\"junk99902\",\"junkieduck113\",\"junkman377\",\"junkman995\",\"katrina\",\"kedits\",\"kingWilly\",\"klingon4real\",\"KronosQuoth\",\"KronosStar\",\"lindyT\",\"livin4HigherPower\",\"lordWally\",\"luvMyPants\",\"luvwool\",\"maha_Homeland\",\"mailfoool\",\"mainman447\",\"mamin\",\"MarcusDrymiau\",\"martaflores\",\"martaO\",\"maskedWoman101\",\"mattdies\",\"meds4realz43\",\"megaMan\",\"michelleR\",\"microBanana\",\"MindOfKronos\",\"mountain478\",\"muppiesRock\",\"NewsOnlineToday\",\"ninjabob\",\"Officia1AbilaPost\",\"omgponies\",\"onl1neRecords\",\"OnlytheTruth\",\"ourcountryourrights\",\"panopticon\",\"partyon\",\"pashmina887\",\"phantomagate\",\"pinky\",\"plasticParts\",\"POK\",\"powercrystals\",\"praise111\",\"prettyRain\",\"protoGuy\",\"pumpitup\",\"redisrad\",\"reggierockin776\",\"ReggieWassali\",\"rnbwBrite\",\"rockinHW\",\"rockStar113\",\"roger_roger\",\"rrWine\",\"Sara_Nespola\",\"sarajane\",\"SaveOurWildlands\",\"shoutItOut\",\"SiaradSea\",\"siliconKing\",\"Simon_Hamaeth\",\"sithLordJames\",\"skinnyJeans\",\"slamrjamr\",\"sofitees\",\"soulofShi\",\"soup4u\",\"starz1134\",\"stolkfair\",\"stuffNstuff\",\"superhero447\",\"supplementsRule\",\"surferMan\",\"teresaJ\",\"trapanitweets\",\"trollingsnark\",\"truccotrucco\",\"truthforcadau\",\"unicorns\",\"vetsRock\",\"Viktor-E\",\"vonneka\",\"whiteprotein\",\"windAvatar\",\"wireHead1122\",\"wiseWords\",\"worldWatcher\",\"writinLazy\",\"yomamma\",\"zengardener\"],\"size\":[1,12,12,15,72,3,20,27,37,2,35,17,6,10,12,30,9,50,4,153,6,16,1,15,5,6,5,25,24,32,4,9,15,3,2,21,2,20,63,25,4,30,4,1,7,4,26,1,3,3,65,5,17,25,2,22,1,25,1,2,1,5,10,10,4,5,7,23,1265,49,21,2,28,22,2,25,3,3,2,8,5,15,10,2,9,106,7,23,9,2,18,21,22,16,1,1,17,23,25,1,3,27,23,23,67,6,4,23,34,1,16,15,14,22,9,4,37,27,11,33,26,2,14,28,9,27,6,32,14,2,23,3,30,2,5,2,2,20,12,25,64,12,3,30,30,26,9,3,24,26,23,4,6,27],\"id\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154],\"x\":[0.999649736802085,-0.0462475772602161,-0.108675215996558,-0.606267377186107,-0.315285277934094,0.317879496433867,-0.331640267716567,-0.143304481021243,-0.421856203786892,-0.684115967035786,-0.520058814386338,-0.284625992386096,0.115835479098042,-0.701921862713659,0.142346394903176,-0.29409637463454,-0.228216045164831,-0.347303872865062,0.0452095959627921,0.83061159391956,0.658882477218922,-0.281775294357652,-0.555639618735419,-0.116041222987497,0.931383196175425,0.666968959407057,-0.01092540473562,-0.116567363521912,-0.449301849867336,-0.14662372540205,-0.910687705167967,0.890999616054375,-0.300280261647223,0.138394103740988,-0.518706190514393,-0.21669644460417,-0.752950589546341,-0.583105569721438,-0.352903837764482,-0.153789983800033,0.577073668172805,-0.23914327365964,-0.185198508712087,0.308921030948675,-0.418582738099694,-0.147704868693545,-0.488086416179166,-0.339863870666109,-0.231068155243668,-0.884936179257481,-0.328035825922841,0.409072681176677,-0.363756778243433,-0.341385617132931,-0.145412521152223,-0.365746669370069,0.231157191798065,-0.428076625069145,0.970556962170857,0.486691258568748,-0.605773302657639,0.726245750983893,-0.392266414631589,0.870962859802878,0.828611806553787,0.551898257375924,0.128069148856737,-0.504704078942881,0.978325302115302,-0.324084602426561,-0.231882828915568,-0.188552126654263,-0.213379227578699,-0.209964332247979,-0.045843377723404,-0.0491015978788003,1,0.00386110504829351,0.0708329314240927,-0.676647848989976,-0.417545348172015,-0.262642007032576,0.647478697794312,0.852975721884149,0.835742487584004,-0.27440607956271,-0.601635194534185,-0.415187634541302,-0.641898379346332,0.231319183186248,-0.588218011508439,-0.285801443906992,-0.340829988524565,-0.304667843067342,-0.536514121138228,0.128288308784384,-0.253973691639031,-0.466544495489341,-0.424254026257815,0.590211007174623,-0.506320701877675,-0.163220778756754,-0.144287838950394,-0.525329164403406,-0.370184263008479,0.0220318979020013,-0.374343923911857,-0.403420604376051,-0.171500775446467,0.988429041533323,0.0176130998515855,-1,-0.724452234781185,-0.169913423791641,0.78552500529564,-0.844762292534996,-0.303492080282659,-0.109854471967791,-0.570091381783779,-0.403605090751252,-0.359981132142798,-0.50641366995648,-0.437300732729272,-0.203745440529129,-0.1251885137291,-0.457653240212891,-0.882102476665372,-0.491640822570742,-0.254790877457676,0.261783059098,-0.447588304461914,0.26311924560206,-0.331830410251084,-0.791424677277795,0.76331412540632,-0.0893374129264338,0.164599038242287,-0.262649700372244,-0.759731957551897,-0.0832248332158154,-0.317783753935675,-0.0128575386003278,0.769502225220648,-0.253512973489399,-0.282665549874912,-0.0902706476756174,0.82568586390647,0.464304616005629,-0.174130991490236,-0.361266620634087,-0.160885199448699,-0.673796721814667,0.757078914998289,-0.496630502208954],\"y\":[0.0247342781558251,-0.585869765145869,-0.521907095974112,-0.309094557572253,-0.316525869083728,0.796632037752418,-0.664939738825927,-0.431972592453739,-0.174073187033936,0.860164761541593,-0.299167138576477,-0.564934912811408,-0.201661055292588,-0.316314260170494,0.975479684722091,-0.32661582911982,0.825840495908886,-0.254982269962741,-0.511861727795013,0.218399291702671,0.294093819896502,-0.166786169579709,0.787352703906503,-0.474562912833211,0.337594587141264,-0.884936988765558,1,-0.223788237408011,-0.240074397844811,-0.319240543859101,-0.107341937961198,-0.648692894715355,-0.477916413478743,0.834840881285717,-0.720515360595628,-0.464274007854643,0.739215819531566,-0.150371528810929,-0.235069664815254,-0.168387433622499,0.62339342927574,-0.218548953948358,0.0896900126750144,0.931104754582647,0.0182155762368481,-0.767583237014716,-0.310499426312998,0.98095970063993,-0.894498731865215,-0.373492559738076,-0.306595957742697,0.657636517884192,-0.285169604279884,-0.191637657488655,-0.89153380824039,-0.461058549492724,-0.109539594717598,-0.399156557448701,0.166471952891049,0.505168391153741,0.213721991721818,0.631763491957437,0.83275049476895,-0.50317994307318,-0.356372610888198,-1,-0.686853344290612,-0.350761360762876,-0.412955562050572,-0.330258642733553,-0.165985012292562,0.997706638007912,-0.410393591954607,-0.322525592596177,0.146668250945083,-0.220820834443075,-0.125655846046008,0.773691920207622,0.0618219520332313,-0.594390316569911,0.266249160756082,-0.0128261594181692,0.467675477915345,-0.0313830310175879,-0.184473068048343,-0.281050249720757,-0.6840050362907,-0.434686829535764,-0.0301123948102261,0.683931652370927,-0.22467601419341,-0.312437039586513,-0.138604339280614,-0.52545493747111,0.167908964084598,0.093997023729042,-0.390308155965428,-0.366357050468148,-0.343867587466463,0.759933286822107,0.250207058286696,-0.222180005422988,-0.369124777316722,-0.373241401991167,-0.3173080092952,0.179155641010003,0.138783478277859,-0.554945220215151,-0.281000652329731,-0.262914284103371,-0.10844782384824,0.56781204747243,-0.416127290527563,-0.134909981150087,0.371218317549812,-0.465515981797224,-0.233366698808181,-0.305445701691039,-0.544194349830244,-0.0959056652218723,-0.375594055233934,0.935640872409454,-0.634077618914525,-0.2592492938575,-0.583439432993548,-0.453994545288131,0.695802515443526,-0.424260546306804,-0.0999552554430431,-0.302310475347925,-0.500040690981048,-0.411781931936821,-0.418439767163715,0.0575033890815286,-0.781129745309307,0.88031989631961,-0.570361362059213,-0.551410072260455,-0.130543327435806,-0.153729025332544,-0.28133488812423,-0.427561680145098,0.087225632192818,-0.488629179169572,-0.423966443990226,-0.323517235469526,0.507431225328778,0.836373168410324,-0.39034060265849,-0.127650053696779,-0.194889658284682,0.0927478412670211,-0.621224601477262,-0.154786865218049]},\"edges\":{\"from\":[2,2,2,2,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,7,7,7,7,13,13,14,14,14,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,19,19,19,24,24,24,24,24,24,24,31,35,35,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,42,42,42,42,42,42,42,42,43,43,45,45,45,46,46,49,50,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,55,57,61,67,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,75,76,76,76,76,76,76,76,76,76,79,80,80,81,82,82,82,82,82,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,87,87,89,89,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,94,94,94,94,94,94,95,96,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,99,99,99,99,99,99,101,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,106,107,107,108,108,108,108,108,108,109,109,109,109,109,109,109,109,109,109,111,111,111,111,113,113,113,116,117,117,117,117,117,117,117,117,117,117,117,117,119,119,119,119,121,121,121,121,121,121,121,121,121,123,123,123,123,123,125,125,125,125,125,129,129,129,129,129,129,130,132,134,137,139,139,140,140,140,140,140,140,140,141,141,141,141,141,141,141,141,141,141,141,141,141,141,141,141,141,141,142,142,142,142,142,145,145,145,145,145,145,145,145,152],\"to\":[73,103,138,149,30,88,102,138,29,58,68,88,120,126,8,9,11,12,22,28,29,30,33,36,40,47,54,56,58,68,71,73,74,88,91,93,102,103,104,114,118,120,124,126,128,131,133,138,144,146,149,150,151,154,12,56,58,73,103,114,58,128,154,8,9,11,12,22,29,30,33,36,40,47,54,58,68,71,73,74,93,102,103,114,118,120,124,126,128,131,133,144,146,149,151,154,28,29,40,47,68,71,73,74,103,104,114,126,149,150,154,103,144,146,36,58,118,138,144,149,151,91,56,126,11,54,58,91,150,9,11,29,30,36,54,88,93,104,114,120,124,133,144,150,154,9,28,30,40,56,88,102,151,102,150,9,124,154,138,144,138,104,8,9,11,12,22,28,29,30,33,36,40,47,54,56,58,68,71,73,74,88,91,93,102,103,104,114,118,120,124,126,128,131,133,144,146,149,150,151,154,8,9,11,22,29,47,54,56,58,68,73,74,88,91,93,102,103,104,114,120,124,128,133,144,149,150,151,154,12,28,120,8,8,9,11,12,22,28,29,33,36,40,47,54,56,58,68,71,73,74,88,91,93,102,103,104,114,118,120,124,126,128,131,133,138,144,146,149,150,151,154,151,8,28,40,54,71,102,114,118,149,151,88,128,120,114,120,146,150,154,8,9,12,22,30,33,40,56,58,68,71,93,102,104,114,118,124,133,146,150,151,154,88,131,9,29,8,9,11,22,28,29,30,36,54,56,68,71,88,93,102,103,104,114,124,126,128,133,138,146,149,150,151,33,36,58,103,128,151,150,114,28,30,54,58,68,118,131,146,9,11,58,104,144,149,150,11,33,91,93,118,138,120,9,12,22,28,29,58,68,74,88,114,120,124,128,131,138,150,114,93,150,29,36,73,88,126,133,28,74,118,120,124,133,138,146,149,150,28,40,102,124,11,68,128,68,9,29,47,54,56,88,93,103,118,120,124,149,11,47,131,138,8,54,73,74,120,128,131,138,150,12,104,128,138,144,8,12,33,73,124,9,28,29,73,74,120,118,146,154,149,11,154,22,30,73,102,146,150,151,9,22,29,33,54,56,71,73,88,102,103,114,120,126,128,144,149,150,8,36,73,74,124,8,9,11,74,133,144,146,149,9]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\",\"label\":\"label\",\"color\":{\"background\":\"lightblue\",\"border\":\"darkblue\",\"highlight\":\"yellow\"},\"physics\":false},\"manipulation\":{\"enabled\":false},\"edges\":{\"smooth\":false},\"physics\":{\"stabilization\":false}},\"groups\":null,\"width\":\"100%\",\"height\":\"400px\",\"idselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":{\"text\":\"Retweet Network\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"highlight\":{\"enabled\":true,\"hoverNearest\":false,\"degree\":1,\"algorithm\":\"all\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":false,\"fit\":false,\"resetHighlight\":true,\"clusterOptions\":null,\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"},\"igraphlayout\":{\"type\":\"square\"}},\"evals\":[],\"jsHooks\":[]}\r\nThis chart can be applied to answer quetions 1 and 2.\r\nFrom the chart, we can tell that the authors in the center like @HomelandIlluminations/@AbilaPost/@KronosStar/@CentralBulletin/@NewsOnlineToday/@InternationalNews have affected the public most directly and widely. And users like @Clevvah4Evah, @KronosQuoth have quite number of junk posting with no re-tweets, while other users with no link post mainly chatter/spam contents.\r\nTo find out how many users are afftected, click the nodes and zoom in the graph, the selected node and its network will be highlighted, which can provided the information of how many people are directly affected by the original author and the size of the dot indicates the total number of posts by users.\r\nQuestion 2\r\n4.2 Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.\r\nNumber of posts\r\n\r\nShow code\r\ncount <- data %>%\r\n  group_by(type,time_1min) %>%\r\n  summarise(count_of_posts= n_distinct(message))\r\n\r\ncount$time_1min=ymd_hms(count$time_1min)\r\n\r\n\r\nmean=mean(count$count_of_posts)\r\n\r\nggplot(count,aes(x=time_1min,y=count_of_posts,fill=type))+\r\n         geom_bar(stat=\"identity\",position=\"dodge\")+\r\n  geom_abline(h=mean, col = \"black\",size=5)+\r\n  #theme(axis.text.x = element_text(angle = 90, hjust = 1))+\r\n  ggtitle(\"Total Number of Posts through Period\")\r\n\r\n\r\nShow code\r\nhead(count[order(-count$count_of_posts),],10)\r\n\r\n\r\n# A tibble: 10 x 3\r\n# Groups:   type [1]\r\n   type   time_1min           count_of_posts\r\n   <chr>  <dttm>                       <int>\r\n 1 mbdata 2014-01-23 19:43:00             61\r\n 2 mbdata 2014-01-23 19:41:00             60\r\n 3 mbdata 2014-01-23 19:44:00             48\r\n 4 mbdata 2014-01-23 19:45:00             48\r\n 5 mbdata 2014-01-23 18:47:00             47\r\n 6 mbdata 2014-01-23 18:45:00             44\r\n 7 mbdata 2014-01-23 18:52:00             38\r\n 8 mbdata 2014-01-23 19:40:00             38\r\n 9 mbdata 2014-01-23 20:10:00             38\r\n10 mbdata 2014-01-23 19:46:00             33\r\n\r\nWith time interval at 1 minute, the sum number of posts is plotted in bar chart as above. The peaks of both ccdata and mbdata are observed during time 19:40-19:50, 18:45:00 and 20:10:00.\r\nTweets and Retweets Trend\r\n\r\nShow code\r\ndata_rt2<-data %>% \r\n   #filter(str_detect(message, \"fire\")) %>% \r\n  select(c(\"author\",\"time_1min\",\"message\",\"RT_from\")) %>% \r\n  group_by(time_1min) %>% \r\n  summarise(post=n(),\r\n            rt_post=sum(RT_from!=\"\"))\r\n\r\ndata_rt2$time_1min=ymd_hms(data_rt2$time_1min)\r\n\r\n\r\n#ggplot(fire,aes(x=time_1min,y=n))+\r\n         #geom_bar(stat=\"identity\",position=\"dodge\")+\r\n  #theme(axis.text.x = element_text(angle = 90, hjust = 1))+\r\n  #ggtitle(\"Total Number of Posts through the period\")\r\n\r\n\r\nggplot(data_rt2,aes(x=time_1min)) +\r\n    geom_bar(aes(y=post), stat = \"identity\",fill = \"red\") +\r\n    geom_bar(aes(y=rt_post), stat = \"identity\",fill = \"blue\") +\r\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\r\n    ggtitle(\"Tweet and Re-Tweets Trend\")\r\n\r\n\r\n\r\n\r\nAnother way to detect the active level of microblog is to visualize the counts of re-tweet/tweet. As shown in the bar graph above, the peak trend coincides with “Total Number of Posts through the period”, indicating the people were actively evolved in the events happening in Abila in particular when number of retweets was high.\r\nThe goverment can monitor the ratio of retweet in community to detect abnormal trend.\r\nMonitor important news source - Mainstream media\r\nNoticed that the mainstream media (with name starting with capital letter and frequently re-tweet by public) are actively quoted media for Abila.\r\n@HomelandIlluminations/@AbilaPost/@KronosStar/@CentralBulletin/@NewsOnlineToday/@InternationalNews are top6 social media. Their tweet should be closely monitored by local authorities, in particular, when the frequency of re-tweet from the public has raised.\r\nFor instance, to understand people’s attention of the abnormal events, we can monitor the words and its relationships in main stream media’s re-tweets.\r\n\r\nShow code\r\n# check if the media is mainstream media\r\n\r\ndata_RT %>% \r\n  filter(RT_from!=author) %>% \r\n  count(RT_from) %>% \r\n  arrange(desc(n)) %>% \r\n  filter(n>40) # filter the retweet over 10 times by public\r\n\r\n\r\n                 RT_from   n\r\n1: HomelandIlluminations 183\r\n2:             AbilaPost 169\r\n3:            KronosStar 143\r\n4:       CentralBulletin  78\r\n5:       NewsOnlineToday  44\r\n6:     InternationalNews  43\r\nShow code\r\n#convert dataframe to corpus\r\n\r\ndata_RT_main<-data %>% \r\n  filter(RT_from==c(\"HomelandIlluminations\",\"AbilaPost\",\r\n                    \"KronosStar\",\"CentralBulletin\",\r\n                    \"NewsOnlineToday\",\"InternationalNews\"))\r\n\r\n\r\ndata_RT_main$id <- seq.int(nrow(data_RT_main))\r\ndata_RT_subset<-data_RT_main %>% select(c(\"id\",\"RT_message\"))\r\n\r\n\r\ndata_RT_subset$RT_message_cleaned<-\r\n  tolower(data_RT_subset$RT_message)%>%   # transform all message to lower cases\r\n  replace_contraction()%>%   #replace contractions with long form\r\n  replace_word_elongation()%>% \r\n  str_replace_all(\"[0-9]\", \"\") %>% #removing numbers\r\n  str_replace_all(\"([,=!.?$+%-&#@])\",\"\")%>% #remove punctuations\r\n  str_replace_all(\"abila|abilapost|centralbulletin|kronosstar|pok|rally\",\"\")%>% \r\n  removeWords(stopwords(\"english\"))%>% \r\n  str_squish()%>% \r\n  str_trim %>% \r\n  lemmatize_strings()\r\n\r\n\r\nx<-data_RT_subset %>% \r\n  unnest_tokens(word, RT_message_cleaned)\r\n\r\nx<-cooccurrence(x, group = \"id\", term = \"word\")\r\n\r\nplt <- textplot_cooccurrence(x,\r\n                             title = \"Re-tweet Words Co-occurrences\", top_n = 100)\r\n\r\nplt\r\n\r\n\r\n\r\n\r\nIn the re-tweet from the main media, high occurrence of fire at dolphin apartment has the most co-occurrence re-tweets, resulting highest attention and risk in the community, followed by several events involved with police such as post of pokrally/ arrive at scene. While in the edge of the visualization, the small group of public was interested in re-tweet firefighter injure at hospital/ suspect arrest, which may be important but have less attention in the community.\r\nQuestion 3\r\n4.3 If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words.\r\nTo send team of respondents to right location with short responding time, we can take a closely look location-tagged posts or posts contents with locations.\r\nRestrospective\r\nOn retrospective approach, I will send team of first respondents to dolphin dancing apartment.\r\nReasons as below,\r\n1)The fire event at dolphin got the most attention and risk from the public according to previous analysis, causing the most injures.\r\n2)In addition, dolphin department was frequently mentioned in the microblog by mainstream media and individual users.\r\n3)Comparing to other event, some are reported by only a few individual users with vague locations mentioned in the latter time, the fire at dolphin apartment has been reported with an accurate location at early time around 18:40. Sending the team there can provide quick and accurate aid.\r\n\r\nShow code\r\nabila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)\r\n\r\n\r\ndata_location <- data %>% \r\n  filter(longitude!=\"\" ) %>% \r\n  add_count(longitude,latitude,author)\r\n\r\ndangermice <- data_location %>% \r\n  filter(author==\"dangermice\" ) %>% \r\n  select(timestamp,message,longitude,latitude) \r\n\r\n\r\np1<-ggplot()+\r\n  geom_sf(data=abila,size=0.2,color=\"black\",fill=\"cyan1\")+\r\n  ggtitle(\"Fire at Dancing Dolphin Apartment\")+\r\n  coord_sf()+\r\n  theme(panel.background = element_rect(fill = \"transparent\"), # bg of the panel\r\n    plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\r\n    panel.grid.major = element_blank(), # get rid of major grid\r\n    panel.grid.minor = element_blank(), # get rid of minor grid\r\n    legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\r\n    legend.box.background = element_rect(fill = \"transparent\"),# get rid of legend panel bg)\r\n    axis.text.x = element_blank(),\r\n  axis.text.y = element_blank(),\r\n  axis.ticks = element_blank(),\r\n  axis.title.y=element_blank(),\r\n  axis.title.x =element_blank()) +\r\n  geom_point(data = dangermice, aes(x = longitude,y=latitude),color=\"red\",size=2)\r\n\r\n\r\n\r\ndata_fire <- data %>% \r\n  filter(str_detect(message,\"fire|explosion|dolpin\")) %>% \r\n  select(message, timestamp,cleaned) %>% \r\n  unnest_tokens(word, cleaned)\r\n\r\ndata_fire_words <- data_fire %>%\r\n  count(word, sort = TRUE) %>%\r\n  ungroup() %>% \r\n  arrange(desc(n))\r\n\r\n\r\n\r\np2<- ggplot(head(data_fire_words,50),aes(label = word,\r\n           size = n,color=n)) +\r\n  geom_text_wordcloud()+\r\n  scale_size_area(max_size = 10) +\r\n  theme(panel.background = element_rect(fill = \"transparent\"), # bg of the panel\r\n    plot.background = element_rect(fill = \"transparent\", color = NA))+\r\n  scale_color_gradient(low = \"darkred\", high = \"red\")\r\n\r\ndata$time_1min<-str_replace_all(data$time_1min,\"2014-01-23 \",\"\")\r\n\r\n\r\n# data$time=ymd_hms(data$time_1min)\r\n# \r\n# p3<-data %>% \r\n#   filter(str_detect(message,\"fire|explosion|dolpin\")) %>% \r\n#   group_by(time) %>% \r\n#   count() %>%\r\n#   ungroup() %>% \r\n#   ggplot(aes(x=time))+\r\n#   geom_bar(aes(y=n), stat = \"identity\",fill = \"black\")+\r\n#   theme(axis.title.y=element_blank(),\r\n#         axis.ticks.y=element_blank(),\r\n#         axis.text.y=element_blank(),\r\n#         axis.title.x=element_blank(),\r\n#         panel.background = element_rect(fill = \"transparent\"),\r\n#         axis.ticks.x=element_blank(),\r\n#         axis.text.x = element_text(angle = 90, hjust = 1))+\r\n#         ggtitle(\"Tweets Trend Related to Dolphin Apartment\")\r\n\r\n\r\n\r\ndata$time=cut(data$timestamp, breaks=\"1 min\")\r\ndata$time =ymd_hms(data$time)\r\n\r\np3<-data %>% \r\n  filter(str_detect(message,\"fire|explosion|dolpin\")) %>% \r\n  group_by(time) %>% \r\n  count() %>%\r\n  ungroup() %>% \r\n  ggplot(aes(x=time))+\r\n  geom_bar(aes(y=n), stat = \"identity\",fill = \"black\")+\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))+\r\n        ggtitle(\"Tweets Trend Related to Dolphin Apartment\")\r\n\r\n\r\n# Move to a new page\r\ngrid.newpage()\r\n# Create layout : nrow = 2, ncol = 2\r\npushViewport(viewport(layout = grid.layout(2, 2)))\r\n# A helper function to define a region on the layout\r\n\r\ndefine_region <- function(row, col){\r\n  viewport(layout.pos.row = row, layout.pos.col = col)\r\n} \r\n# Arrange the plots\r\nprint(p1, vp=define_region(1, 1))\r\nprint(p2, vp = define_region(1, 2))\r\nprint(p3, vp = define_region(2, 1:2))\r\n\r\n\r\n\r\n\r\nReal-Time\r\nIf it’s real time, I will arrange the trajectory of the team according to the status of the event and its location\r\nIn the real time events, I will monitor the tweets/retweets trend, risk level and location-stamped posts.\r\nAt 19:20:00 pm, hennyhenhendrix posted a location-stamped post -“Some moron in a black van just hit my car!” then, in 2 minutes, all call center alramed - “ALL UNITS BROADCAST-FELONY HIT & RUN-BLACK VAN/PARTIAL PLATE #L829” which brings the event to very high attention. The team should be sent to the location of 547 N. Schaber Ave as reporeted by call center. And the team should follow the trajectory of the location based by call center reporting the same contents,\r\nEgeou Ave / Ipsilantou Ave - N. Hacia St / N. Brada St -N. Antoniadou St / N. Estos St, Profitou Ilia St / Egeou Ave, and N. Limnou St / N. Achilleos St ( 7:38:22 pm)\r\nAt the same time, it is suggested that authority keep monitoring the location-stamped post in the community to keep tracking the black van. Users like truccotrucco and sofitees were key reporters located near to N. Limnou St / N. Achilleos St reporting the hostage regarding black van. The team should notice and follow their location and provide first aid accordingly.\r\n\r\nShow code\r\nabila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)\r\n\r\n\r\ndf_ccdata<-data %>% \r\n  filter(type==\"ccdata\",location!=\"N/A\",str_detect(message,\"POLICE|FIRE|CRIME|SUSPICIOUS|VAN\"))\r\n\r\n\r\ndf_ccdata<-subset(df_ccdata,select=c(\"type\",\"message\",\"location\",\"timestamp\")) \r\n\r\ndf_ccdata<-df_ccdata %>%separate(location,c(\"street1\",\"street2\"),sep=\"(/)\",convert=T) %>% \r\n  drop_na(street2)\r\n\r\ndf_ccdata<-df_ccdata %>% \r\n  separate(street1,c(\"street1_D\",\"street1\"),sep=\"\\\\.\",convert=F)\r\n\r\n\r\ndf_ccdata<-df_ccdata %>% \r\n  separate(street2,c(\"street2_D\",\"street2\"),sep=\"\\\\.\",convert=F)\r\n\r\ndf_ccdata$street1<-ifelse(is.na(df_ccdata$street1),df_ccdata$street1_D,df_ccdata$street1)\r\ndf_ccdata$street1_D<-ifelse(df_ccdata$street1==df_ccdata$street1_D,NA,df_ccdata$street1_D)\r\n\r\ndf_ccdata$street2<-ifelse(is.na(df_ccdata$street2),df_ccdata$street2_D,df_ccdata$street2)\r\ndf_ccdata$street2_D<-ifelse(df_ccdata$street2==df_ccdata$street2_D,NA,df_ccdata$street2_D)\r\n\r\n\r\ndf_ccdata$street1 <- str_replace_all(df_ccdata$street1,\" [St|Ave]\",\"\")\r\ndf_ccdata$street1 <- str_replace_all(df_ccdata$street1,\" \",\"\")\r\ndf_ccdata$street2 <- str_replace_all(df_ccdata$street2,\" [St|Ave]\",\"\")\r\ndf_ccdata$street2 <- str_replace_all(df_ccdata$street2,\" \",\"\")\r\n\r\n# Read Abila Map\r\nabila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)\r\n\r\np = npts(abila, by_feature = TRUE)\r\nabila <- cbind(abila, p) %>%\r\n  filter(p>1)\r\nabila<-abila %>% mutate(geometry2=st_touches(geometry))\r\n\r\n\r\nl1<-c(\"184646017\",\"184646189\")\r\nl11<-c(\"184646189\",\"184646726\")\r\nl2<-c(\"184646726\",\"184645566\")\r\nl3<-c(\"184645566\",\"184645397\")\r\nl4<-c(\"184645397\",\"184644393\")\r\n\r\nl5<-c(\"184646017\",\"184646189\",\"184646726\",\"184645566\",\"184645397\",\"184645397\",\"184644393\")\r\n\r\n\r\nabila1<-abila %>%\r\nfilter(TLID %in% l1)\r\np= npts(abila1, by_feature = TRUE)\r\nabila1 <- cbind(abila1, p) %>%filter(p>1) \r\nabila1<-abila1%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\nabila11<-abila %>%\r\nfilter(TLID %in% l11)\r\np= npts(abila1, by_feature = TRUE)\r\nabila11 <- cbind(abila11, p) %>%filter(p>1) \r\nabila11<-abila11%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\nabila2<-abila %>%\r\nfilter(TLID %in% l2)\r\np= npts(abila2, by_feature = TRUE)\r\nabila2 <- cbind(abila2, p) %>%filter(p>1) \r\nabila2<-abila2%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\nabila3<-abila %>%\r\nfilter(TLID %in% l3)\r\np= npts(abila3, by_feature = TRUE)\r\nabila3 <- cbind(abila3, p) %>%filter(p>1) \r\nabila3<-abila3%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\nabila4<-abila %>%\r\nfilter(TLID %in% l4)\r\np= npts(abila4, by_feature = TRUE)\r\nabila4 <- cbind(abila4, p) %>%filter(p>1) \r\nabila4<-abila4%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\nabila5<-abila %>%\r\nfilter(TLID %in% l5)\r\np= npts(abila5, by_feature = TRUE)\r\nabila5 <- cbind(abila5, p) %>%filter(p>1) \r\nabila5<-abila5%>% \r\nmutate(geometry2=st_touches(geometry)) %>%  \r\n  mutate(long = unlist(map(geometry,2)),\r\n           lat = unlist(map(geometry,3)))\r\n\r\n\r\ndata_location <- data %>% \r\n  filter(longitude!=\"\",str_detect(message,\"hit|van|shoot|driver\") ) %>% \r\n  add_count(longitude,latitude,author) %>% \r\n  rename(Witness=author)\r\n\r\n\r\nabila5$label=c(\"van observation\",\"van observation\",\"van observation\",\"fire\",\"car/bike hit\",\"van observation\")\r\n\r\n\r\nggplot()+\r\n  geom_sf(data=abila,size=1,color=\"grey\",fill=\"cyan1\")+\r\n  ggtitle(\"Abila\")+\r\n  geom_point(data = data_location, aes(x = longitude,y=latitude,color=Witness),size=3)+\r\n  geom_point(data = abila5,mapping = aes(x=long,y=lat,label=label),color=\"red\",size=3)+\r\n  geom_text(data = abila5,mapping = aes(x=long,y=lat,label=label),color=\"red\",size=3,vjust=2,hjust=-0.1)+\r\n  theme(axis.title.y=element_blank(),\r\n        axis.ticks.y=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        axis.title.x=element_blank(),\r\n        panel.background = element_rect(fill = \"transparent\"),\r\n        axis.text.x=element_blank(),\r\n        axis.ticks.x=element_blank())+\r\n        ggtitle(\"Located Ccdata and Microblogs for Real-time Tracking\")+\r\n  geom_text()+\r\n  geom_line(data = abila1,mapping = aes(x=long,y=lat),color=\"red\",size=1,linetype = \"dashed\")+\r\n  #geom_text(aes(x = 0, y = 0, label = \"AAPL\"))+\r\n  geom_line(data = abila11,mapping = aes(x=long,y=lat),color=\"red\",size=1,linetype = \"dashed\")+\r\n  geom_line(data = abila2,mapping = aes(x=long,y=lat),color=\"red\",size=1,linetype = \"dashed\")+\r\n  geom_line(data = abila3,mapping = aes(x=long,y=lat),color=\"red\",size=1,linetype = \"dashed\")+\r\n  geom_line(data = abila4,mapping = aes(x=long,y=lat),color=\"red\",size=1,linetype = \"dashed\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\ndata_location$timestamp<-format(data_location$timestamp, format = \"%H:%M:%S\")\r\n\r\nDT::datatable(subset(data_location,select=c(timestamp,Witness,message,longitude,latitude)))\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\"],[\"19:20:00\",\"19:20:00\",\"19:21:01\",\"19:21:38\",\"19:22:00\",\"19:39:40\",\"19:40:00\",\"19:40:41\",\"19:41:03\",\"19:41:33\",\"19:42:28\",\"19:43:11\",\"19:43:39\",\"19:44:23\",\"19:44:38\",\"19:45:11\",\"19:46:00\",\"19:50:00\",\"19:51:13\",\"19:55:59\",\"19:58:13\",\"20:11:21\",\"21:00:04\",\"21:00:04\",\"21:04:05\",\"21:13:00\",\"21:14:01\",\"21:17:11\",\"21:21:11\"],[\"hennyhenhendrix\",\"brewvebeenserved\",\"brewvebeenserved\",\"brewvebeenserved\",\"trapanitweets\",\"trapanitweets\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"hennyhenhendrix\",\"truccotrucco\",\"sofitees\",\"truccotrucco\",\"truccotrucco\",\"truccotrucco\",\"sofitees\",\"truccotrucco\"],[\"Some moron in a black van just hit my car!\",\"WOW! Crazy driver! Hopefully everyone is ok!\",\"Oh no!! That crazy driver hit a guy on a bike outside our shop!!\",\"FREE COFFEE to anyone who caught the license plate of that black van! #jerkdrivers\",\"OMG!!!!!!! Some derp in a?balck?van just?hit a guy on a bike!!!!!\",\"Except for that derp who hit henri.\",\"wow some crazy black van just got pulled over in the parking lot - dinner &amp; a show!\",\"OMG there shooting there shooting!\",\"what is happening? they r right out front shooting\",\"why didnt i stay at the rally? im trapped in here the van is right outsdie the door\",\"havent heard any more shots pls no more #shooting\",\"who shoots a cop? who are these maniacs?!?!\",\"im hearing more sirens i think i can get a view outside #shooting\",\"looks like there are 3 cop cars and the black van two people in the front of van #shooting\",\"putting my head back down #shooting\",\"i think were trapped in here #gelatogalore #shooting\",\"more cops have arrived someone just saw #shooting\",\"ambulance has arrived hopefully getting the cop to the hospital fast praying hes OK #shooting\",\"#APD are heroes! #shooting\",\"i think the swat team has arrived hopefully stand off ends soon #shooting #standoff\",\"guy in the van keeps yeelling at cops cant hear him but looks mad #standoff\",\"they seemed really interested in where that van came from\",\"what is this guy doing? just give up and now hes back in the van #standoff #gelatogalore\",\"crazy guy is back in the van - what is going on?\",\"looks like therye still fighting in the van really animated #standoff\",\"someones out of the van! is it over?\",\"other guy is out of the van! its over theyre giving up! #standoff\",\"i think there giving up 2 people hands up got out of the van guy &amp; girl\",\"cant belive it a peaceful ending after a shootout! #stillshaking\"],[24.892,24.901,24.901,24.901,24.901,24.901,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.856,24.892,24.856,24.857,24.856,24.856,24.856,24.857,24.856],[36.057,36.054,36.054,36.054,36.054,36.054,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.059,36.057,36.059,36.059,36.059,36.059,36.059,36.059,36.059]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>timestamp<\\/th>\\n      <th>Witness<\\/th>\\n      <th>message<\\/th>\\n      <th>longitude<\\/th>\\n      <th>latitude<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[4,5]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nThe map has highlighted the black van possible routine reported by call center and witness location-stamped tweet. The table tracks the real-time location-stamped tweet contents.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:32+08:00"
    },
    {
      "path": "Reference.html",
      "title": "5. Reference ",
      "author": [
        {
          "name": "Linya Huang",
          "url": {}
        },
        {
          "name": {},
          "url": {}
        }
      ],
      "date": "07-25-2021",
      "contents": "\r\n\r\nContents\r\nAcknowledgement\r\nReference\r\n\r\nAcknowledgement\r\nThis assignment is guided by Prof. Kam Tin Seong\r\nReference\r\n• Junghoon Chae, Dennis Thom, Yun Jang, SungYe Kim, Thomas Ertl, David S. Ebert, Public behavior response analysis in disaster events utilizing visual analytics of microblog data,Computers & Graphics,Volume 38,2014,Pages 51-60,ISSN 0097-8493,https://doi.org/10.1016/j.cag.2013.10.008.\r\n• Biterm Topic Model\r\n• NLP in R: Topic Modeling\r\n• timevis\r\n• Textnets\r\n• Text Mining with R\r\n• Visual Analytics Benchmark Repository-VAST Challenge 2014\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T23:15:33+08:00"
    }
  ],
  "collections": []
}
