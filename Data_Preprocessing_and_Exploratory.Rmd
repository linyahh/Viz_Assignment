---
title: "3. Data Preprocessing and Exploratort Data Analysis"
description: |
  In this section, I will apply text mining techniques to perform data preprocessing and conduct exploratory data analysis.
author:
  - name: Linya Huang
  - affiliation: School of Computing and Information Systems, Singapore Management University
date: 07-25-2021
output:
  distill::distill_article:
    code_folding: true
    toc: true
    toc_depth: 6
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3,
  echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning=FALSE)
```

## 3.1 Import packages and social media stream data

### Import Packages


```{r}
packages= c(
            'tidyverse','data.table','lubridate',
            'textclean','tm','wordcloud','text2vec',
            'topicmodels','tidytext','textmineR','quanteda',
            'BTM','textplot','concaveman','ggwordcloud',
            'qdapDictionaries','textstem','devtools','textnets',
            'igraph', 'tidygraph', 
             'ggraph', 'visNetwork','udpipe','grid','sp','sf','gridExtra','mapview','tmap')

for(p in packages){
  if(!require(p,character.only= T)){
    install.packages(p)
    }
  library(p, character.only = T)
}
```

Main R packages applied:

__1) Data Cleaning__

'tidyverse', 'lubridate'

__2) Text Data Processing__

'textclean','tidytext','BTM','topicmodels','textmineR',

__3) Data Visualization__ 

_Basic Charts-_

'tidyverse'(ggplot)

_Text-_

'textplot','ggwordcloud','wordcloud','textstem','textnets'

_Network-_

'ggraph', 'visNetwork'

_Map-_

'sp','sf','tmap','gridExtra','mapview'


### Import Dataset

Import and append data set.
```{r}
#read csv file
data_17_1830=read_csv("data/MC3/csv-1700-1830.csv")
data_1830_20=read_csv("data/MC3/csv-1831-2000.csv")
data_20_2130=read_csv("data/MC3/csv-2001-2131.csv")

#append 3 dataset

data=rbindlist(list(data_17_1830,data_1830_20,data_20_2130))

#print head of data
knitr::kable(head(data,1), "pipe")

```


## 3.2 Manipulate Datetime 

Change datetime string to timestamp by using lubridate package.

```{r, echo=TRUE}

#timestamp in lubridate
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_1min = cut(data$timestamp, breaks="1 min")
data$time_30min = cut(data$timestamp, breaks="30 min")
```

 
## 3.3 Text Data Preprocessing


Preprocess text data by lower case transformation, remove punctuation, numbers , stopwords and retrieve RT/ hashtags from message by using regex techniques.

```{r}
data$id <- seq.int(nrow(data))


ppl <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
rt <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"




data$cleaned<-data$message %>% 
  str_replace_all(rt,"")%>%
  str_replace_all(ppl,"")%>%
  str_replace_all(hash,"") %>% 
  tolower()%>%   # transform all message to lower cases
  replace_contraction()%>%   #replace contractions with long form
  replace_word_elongation()%>% #remove the same letter (case insensitive) appears 3 times consecutively
  str_replace_all("[0-9]", "") %>% #removing numbers
  str_replace_all("([,=!.?$+%-&])","")%>% #remove punctuations
  #str_replace_all("rt|pokrally|kronosstar","")%>%
  #|#hi|#pok|#pokrally|
  # #abilapost|#kronosstar|#centralbulletin|@centralbulletin|@kronosstar|rally|aliba") #remove hashtag and rt
  removeWords(stopwords("english"))%>% 
  str_squish()%>% #trim whitespace from a string 
  lemmatize_strings()#removes whitespace from start and end of string



# Retrieve RT/ hashtags from message by using regex techniques

regex <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex2 <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex3 <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
regex4 <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"

data$RT_pattern<-str_extract_all(data$message, regex, simplify = TRUE)
data$RT_from<-  str_extract_all(data$RT_pattern, regex2, simplify = TRUE)
data$RT_from<-  str_replace(data$RT_from, "@","")
data$RT_message <- str_replace_all(data$message,regex3,"")


data$user_mentioned <- str_extract_all(data$message, regex2, simplify = FALSE)


data$user_mentioned<-data$user_mentioned%>% 
  str_replace_all("character\\(0\\)|c\\(|\\)|,","")

data_RT<- data %>% 
  filter(RT_from!="")

knitr::kable(head(subset(data,select=c("message","cleaned"))), "pipe")

```


Get Hashtags.

```{r}
GetHashtags <- function(tweet) {

  hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>% 
    as.character()
  
  hashtag.string <- NA
  
  if (length(hashtag.vector) > 0) {
    
    hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
    
  } 

  return(hashtag.string)
}

hashtags.df <- tibble(
  Hashtags = data$message %>% map_chr(.f = ~ GetHashtags(tweet = .x))
)
hashtags.df %>% head()
```

## 3.4 EDA

### Text Data Word Cloud

Use world cloud to show most frequent words/hastags/tagged users.
 
```{r,warning=FALSE}

#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$cleaned)))
#inspect(docs[1:2])

# Create a document-term-matrix

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 

# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)

#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')


#convert dataframe to corpus

hashtags.df<-na.omit(hashtags.df) 

hashtags.df$id<-seq(nrow(hashtags.df))

hashtags.df$Hashtags<-str_remove(hashtags.df$Hashtags," ")


lst = strsplit(hashtags.df$Hashtags, ",")
names(lst) = hashtags.df$id
hashtags.df2 = melt(lst)

hashtags.df2<-hashtags.df2 %>% group_by(value) %>% count() %>% ungroup()

wordcloud(words = hashtags.df2$value ,freq = hashtags.df2$n, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))


#convert dataframe to corpus


docs <- Corpus(VectorSource(as.character(data$user_mentioned)))
#inspect(docs[1:2])

# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 

# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)

#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')
```
### Messages from Call Center

Visualize count of distinct messages from call center. The most frequent messages from call center are traffic stop, incomplete call for police and alarm-secure no crime. And there are numbers of messages related to suspicious as well.

```{r,fig.width=12,fig.height= 12}
cc_data<-select(data,"type", "message","timestamp","location")%>% 
  filter(type=="ccdata")

cc_data2<-cc_data %>% 
  group_by(message) %>%
  count() %>% 
  arrange(desc(n))


ggplot(cc_data2,aes(x=reorder(message,n), y=n)) +
  geom_bar(stat='identity') +
  coord_flip()+
  ggtitle("Messages from Call Center")+
  theme_classic()+
  geom_text(aes(label = n),hjust=1,colour = "white")


```



