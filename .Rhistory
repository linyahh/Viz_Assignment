#wordcloud2(data=df, size=1.6, color='random-dark')
cc_data<-select(data,"type", "message","timestamp","location")%>%
filter(type=="ccdata")
cc_data2<-cc_data %>%
group_by(message) %>%
count() %>%
arrange(desc(n))
ggplot(cc_data2,aes(x=reorder(message,n), y=n)) +
geom_bar(stat='identity') +
coord_flip()+
ggtitle("Messages from Call Center")+
theme_classic()+
geom_text(aes(label = n),hjust=1,colour = "white")
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning=FALSE)
packages= c('sf','clock','tmap',
'tidyverse','data.table','lubridate',
'textclean','tm','wordcloud','text2vec',
'topicmodels','tidytext','textmineR','quanteda',
'BTM','textplot','concaveman','ggwordcloud',
'textstem','devtools','textnets',
'ggiraph','plotly','igraph', 'tidygraph',
'ggraph', 'visNetwork','udpipe','grid')
for(p in packages){
if(!require(p,character.only= T)){
install.packages(p)
}
library(p, character.only = T)
}
#add id to dataset
data$id <- seq.int(nrow(data))
data_subset=subset(data,select=c("time_30min","cleaned"))
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
data_subset$time_30min<-str_replace_all(data_subset$time_30min,"2014-01-23 21:30:00","2014-01-23 21:00:00")
bigrams <- data_subset%>%
group_by(time_30min)%>%
unnest_tokens(word,
cleaned,
token = "ngrams",
n = 2) %>%
count(time_30min,word, sort = TRUE) %>% ungroup()
# bigrams <- data_subset%>%
#   unnest_tokens(word,
#                 cleaned,
#                 token = "ngrams",
#                 n = 2) %>%
#   count(time_30min,word, sort = TRUE)
tf_idf <- bigrams%>%
bind_tf_idf(word,time_30min, n) %>%
arrange(desc(tf_idf))
tf_idf$time_30min<-str_replace_all(tf_idf $time_30min,"2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
tf_idf<-left_join(tf_idf,time_30min_df,by=c("time_30min"="l1"))
tf_idf %>%
group_by(Time_30min) %>%
slice_max(tf_idf,
n = 10) %>%
ungroup() %>%
mutate(word = reorder(word,
tf_idf)) %>%
ggplot(aes(tf_idf,
word,
fill = Time_30min)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ Time_30min,
scales = "free") +
labs(x = "TF-IDF Bigram in 30 time interval",
y = NULL)
wordcorpus <- Corpus(VectorSource(as.character(data$cleaned)))
dtm <- DocumentTermMatrix(wordcorpus,
control = list(
wordLengths=c(2, Inf),               # limit word length
bounds = list(global = c(5,Inf)),    # minimum word frequency
removeNumbers = TRUE,                #remove Numbers
weighting = weightTf,                #weighted term frequency
encoding = "UTF-8"))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove 0 dtm rows of matrix
topic=LDA(dtm.new,k=10,method="Gibbs",conrol=list(seed=2021,alpha=0.01,iter=300))
terms(topic,5)
ap_topics <- tidy(topic, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
topic_gamma <- tidy(topic, matrix = "gamma")
topic_gamma <- topic_gamma %>%
group_by(document) %>%
slice(which.max(gamma))
topic_gamma$document<-as.numeric(topic_gamma$document)
#topic_gamma[order(topic_gamma$document),] %>% group_by(topic) %>% count()
#(topic_gamma%>% arrange(desc(-document)))
id_time <- data %>% select(c("id","time_1min"))
topic_data<-left_join(topic_gamma,id_time,by=c("document"="id"))
#manually put topics in LDA results
topic_c<- c(1,2,3,4,5,6,7,8,9,10)
topics_c <- c("police","chatter1","chatter2","fire","chatter3",
"van","chatter4","chatter5","pokrally","pokleader")
topic_df<-data.frame(topic_c,topics_c )
topic_data<-left_join(topic_data,topic_df,by=c("topic"="topic_c"))
topic_data %>% group_by(time_1min,topics_c) %>% count() %>%
ggplot(aes(x=time_1min))+
geom_bar(aes(y=n), stat = "identity",fill = "black")+
facet_wrap(~topics_c)+
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())+
ggtitle("Topics Trend from 1700-2130")
#Tokenize data
tidytxtdata<- tidy(dtm)
tidytxtdata <- tidytxtdata%>% #Remove the count column
select(-count)
tidytxtdata <- tidytxtdata%>% #Change the column name 'term' to 'word' so that we can get rid of stopwords later
rename(word = term)
#Remove stopwords
tidytxtdata <- tidytxtdata%>%
anti_join(stop_words)
#Use the btm model
set.seed(321)
model <- BTM(tidytxtdata, k = 10, beta = 0.01, background = TRUE, iter = 500, trace = 100) #Run the model
topicterms <- terms(model, top_n = 10) #View the topics
#topicterms
library(textplot)
library(ggraph)
library(concaveman)
plot(model)
tweet<-data %>%
filter(author!="NA") %>%
group_by(author) %>%
count() %>%
ungroup()
retweet<-data %>%
group_by(RT_from) %>%
count()%>%
ungroup()
repeat_data<-data %>%
filter(author!="NA") %>%
group_by(author,message) %>%
count() %>%
ungroup() %>%
group_by(author) %>%
count() %>%
ungroup()
colnames(tweet)[2]<-"tweet"
colnames(retweet)[2]<-"retweet"
colnames(repeat_data)[2]<-"repeat"
tweet_retweet<-left_join(tweet, retweet, by = c("author"="RT_from"))
tweet_retweet<-left_join(tweet_retweet, repeat_data, by = c("author"="author"))
tweet_retweet$retweet_ratio<- tweet_retweet$retweet/tweet_retweet$tweet
tweet_retweet$unique_ratio <-tweet_retweet$'repeat'/tweet_retweet$tweet
tweet_retweet$retweet<-ifelse(is.na(tweet_retweet$retweet),0,tweet_retweet$retweet)
tweet_retweet$retweet_ratio<-ifelse(is.na(tweet_retweet$retweet_ratio),0,tweet_retweet$retweet_ratio)
tweet_retweet_2<-tweet_retweet %>% filter(unique_ratio<0.8 | retweet_ratio>=2)
ggplot(data=tweet_retweet_2,aes(x=reorder(author,tweet)))+
geom_bar(aes(y=tweet),stat = "identity",fill="blue",alpha=0.5)+
geom_bar(aes(y=retweet),stat = "identity",fill="red",alpha=0.4)+
geom_point(mapping=aes( y=unique_ratio*1300), color="black",size=2)+
scale_y_continuous(limits=c(0, 1300),name = "Number of Tweets/Re-Tweets",sec.axis = sec_axis(~ ./1300,name = "Unique Ratio"))+
theme(axis.text.x = element_text(size=8, angle=45),
panel.border = element_blank(),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_line(colour = "black"))+
ggtitle("Tweets/Retweets and Unique Tweets Ratio")
# dt<-subset(data,select=c(timestamp,author,message))
# dt$timestamp<-str_replace_all(dt$timestamp,"2014-01-23 ","")
# dt$message<-iconv(dt$message, 'utf-8', 'ascii', sub='')
# tweet_retweet_2<-subset(tweet_retweet,select=c(author,tweet,retweet))
# dt<-left_join(dt,tweet_retweet_2,by=c("author"="author"))
# dt<-dt %>% filter(author!="")
# # dt<-dt%>% mutate_if(is.character, ~gsub('[^ -~]', '', .)) # remove characters non UTF-8
# DT::datatable(dt)
RT_edges_aggregated <-data_RT%>%
group_by(RT_from,author) %>%
count() %>%
ungroup()
RT_nodes_aggregated <-data%>%
group_by(author) %>%
count() %>%
ungroup
RT_nodes_aggregated$id<-seq.int(nrow(RT_nodes_aggregated))
RT_nodes_aggregated<-RT_nodes_aggregated %>%
rename(label=author,size=n) %>%
filter(label!="")
#RT_nodes_aggregated$label<-paste(RT_nodes_aggregated$label,RT_nodes_aggregated$size)
RT_edges_aggregated_viz<-
left_join(RT_edges_aggregated,RT_nodes_aggregated,by=c("RT_from"="label")) %>%
rename(from=id) %>%
left_join(RT_nodes_aggregated,by=c("author"="label")) %>%
rename(to=id)
RT_edges_aggregated_viz<-subset(RT_edges_aggregated_viz,select=c("from","to"))
RT_graph <- tbl_graph(nodes=RT_nodes_aggregated,
edges = RT_edges_aggregated,
directed = TRUE)
visNetwork(RT_nodes_aggregated,
RT_edges_aggregated_viz ,
main = "Retweet Network",width="100%", height="400px")%>%
visOptions(highlightNearest = TRUE)%>%
visNodes(label="label",color = list(background = "lightblue",
border = "darkblue",
highlight = "yellow"))%>%
visIgraphLayout(layout = "layout_with_fr")
count <- data %>%
group_by(type,time_1min) %>%
summarise(count_of_posts= n_distinct(message))
count$time_1min=ymd_hms(count$time_1min)
mean=mean(count$count_of_posts)
ggplot(count,aes(x=time_1min,y=count_of_posts,fill=type))+
geom_bar(stat="identity",position="dodge")+
geom_abline(h=mean, col = "black",size=5)+
#theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ggtitle("Total Number of Posts through Period")
head(count[order(-count$count_of_posts),],10)
data_rt2<-data %>%
#filter(str_detect(message, "fire")) %>%
select(c("author","time_1min","message","RT_from")) %>%
group_by(time_1min) %>%
summarise(post=n(),
rt_post=sum(RT_from!=""))
data_rt2$time_1min=ymd_hms(data_rt2$time_1min)
#ggplot(fire,aes(x=time_1min,y=n))+
#geom_bar(stat="identity",position="dodge")+
#theme(axis.text.x = element_text(angle = 90, hjust = 1))+
#ggtitle("Total Number of Posts through the period")
ggplot(data_rt2,aes(x=time_1min)) +
geom_bar(aes(y=post), stat = "identity",fill = "red") +
geom_bar(aes(y=rt_post), stat = "identity",fill = "blue") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ggtitle("Tweet and Re-Tweets Trend")
# check if the media is mainstream media
data_RT %>%
filter(RT_from!=author) %>%
count(RT_from) %>%
arrange(desc(n)) %>%
filter(n>40) # filter the retweet over 10 times by public
#convert dataframe to corpus
data_RT_main<-data %>%
filter(RT_from==c("HomelandIlluminations","AbilaPost",
"KronosStar","CentralBulletin",
"NewsOnlineToday","InternationalNews"))
data_RT_main$id <- seq.int(nrow(data_RT_main))
data_RT_subset<-data_RT_main %>% select(c("id","RT_message"))
data_RT_subset$RT_message_cleaned<-
tolower(data_RT_subset$RT_message)%>%   # transform all message to lower cases
replace_contraction()%>%   #replace contractions with long form
replace_word_elongation()%>%
str_replace_all("[0-9]", "") %>% #removing numbers
str_replace_all("([,=!.?$+%-&#@])","")%>% #remove punctuations
str_replace_all("abila|abilapost|centralbulletin|kronosstar|pok|rally","")%>%
removeWords(stopwords("english"))%>%
str_squish()%>%
str_trim %>%
lemmatize_strings()
x<-data_RT_subset %>%
unnest_tokens(word, RT_message_cleaned)
x<-cooccurrence(x, group = "id", term = "word")
plt <- textplot_cooccurrence(x,
title = "Re-tweet Words Co-occurrences", top_n = 100)
plt
abila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)
data_location <- data %>%
filter(longitude!="" ) %>%
add_count(longitude,latitude,author)
dangermice <- data_location %>%
filter(author=="dangermice" ) %>%
select(timestamp,message,longitude,latitude)
p1<-ggplot()+
geom_sf(data=abila,size=0.2,color="black",fill="cyan1")+
ggtitle("Fire at Dancing Dolphin Apartment")+
coord_sf()+
theme(panel.background = element_rect(fill = "transparent"), # bg of the panel
plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
panel.grid.major = element_blank(), # get rid of major grid
panel.grid.minor = element_blank(), # get rid of minor grid
legend.background = element_rect(fill = "transparent"), # get rid of legend bg
legend.box.background = element_rect(fill = "transparent"),# get rid of legend panel bg)
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank(),
axis.title.y=element_blank(),
axis.title.x =element_blank()) +
geom_point(data = dangermice, aes(x = longitude,y=latitude),color="red",size=2)
data_fire <- data %>%
filter(str_detect(message,"fire|explosion|dolpin")) %>%
select(message, timestamp,cleaned) %>%
unnest_tokens(word, cleaned)
data_fire_words <- data_fire %>%
count(word, sort = TRUE) %>%
ungroup() %>%
arrange(desc(n))
p2<- ggplot(head(data_fire_words,50),aes(label = word,
size = n,color=n)) +
geom_text_wordcloud()+
scale_size_area(max_size = 10) +
theme(panel.background = element_rect(fill = "transparent"), # bg of the panel
plot.background = element_rect(fill = "transparent", color = NA))+
scale_color_gradient(low = "darkred", high = "red")
data$time_1min<-str_replace_all(data$time_1min,"2014-01-23 ","")
# data$time=ymd_hms(data$time_1min)
#
# p3<-data %>%
#   filter(str_detect(message,"fire|explosion|dolpin")) %>%
#   group_by(time) %>%
#   count() %>%
#   ungroup() %>%
#   ggplot(aes(x=time))+
#   geom_bar(aes(y=n), stat = "identity",fill = "black")+
#   theme(axis.title.y=element_blank(),
#         axis.ticks.y=element_blank(),
#         axis.text.y=element_blank(),
#         axis.title.x=element_blank(),
#         panel.background = element_rect(fill = "transparent"),
#         axis.ticks.x=element_blank(),
#         axis.text.x = element_text(angle = 90, hjust = 1))+
#         ggtitle("Tweets Trend Related to Dolphin Apartment")
data$time=cut(data$timestamp, breaks="1 min")
data$time =ymd_hms(data$time)
p3<-data %>%
filter(str_detect(message,"fire|explosion|dolpin")) %>%
group_by(time) %>%
count() %>%
ungroup() %>%
ggplot(aes(x=time))+
geom_bar(aes(y=n), stat = "identity",fill = "black")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ggtitle("Tweets Trend Related to Dolphin Apartment")
# Move to a new page
grid.newpage()
# Create layout : nrow = 2, ncol = 2
pushViewport(viewport(layout = grid.layout(2, 2)))
# A helper function to define a region on the layout
define_region <- function(row, col){
viewport(layout.pos.row = row, layout.pos.col = col)
}
# Arrange the plots
print(p1, vp=define_region(1, 1))
print(p2, vp = define_region(1, 2))
print(p3, vp = define_region(2, 1:2))
abila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)
df_ccdata<-data %>%
filter(type=="ccdata",location!="N/A",str_detect(message,"POLICE|FIRE|CRIME|SUSPICIOUS|VAN"))
df_ccdata<-subset(df_ccdata,select=c("type","message","location","timestamp"))
df_ccdata<-df_ccdata %>%separate(location,c("street1","street2"),sep="(/)",convert=T) %>%
drop_na(street2)
df_ccdata<-df_ccdata %>%
separate(street1,c("street1_D","street1"),sep="\\.",convert=F)
df_ccdata<-df_ccdata %>%
separate(street2,c("street2_D","street2"),sep="\\.",convert=F)
df_ccdata$street1<-ifelse(is.na(df_ccdata$street1),df_ccdata$street1_D,df_ccdata$street1)
df_ccdata$street1_D<-ifelse(df_ccdata$street1==df_ccdata$street1_D,NA,df_ccdata$street1_D)
df_ccdata$street2<-ifelse(is.na(df_ccdata$street2),df_ccdata$street2_D,df_ccdata$street2)
df_ccdata$street2_D<-ifelse(df_ccdata$street2==df_ccdata$street2_D,NA,df_ccdata$street2_D)
df_ccdata$street1 <- str_replace_all(df_ccdata$street1," [St|Ave]","")
df_ccdata$street1 <- str_replace_all(df_ccdata$street1," ","")
df_ccdata$street2 <- str_replace_all(df_ccdata$street2," [St|Ave]","")
df_ccdata$street2 <- str_replace_all(df_ccdata$street2," ","")
# Read Abila Map
abila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)
p = npts(abila, by_feature = TRUE)
abila <- cbind(abila, p) %>%
filter(p>1)
abila<-abila %>% mutate(geometry2=st_touches(geometry))
l1<-c("184646017","184646189")
l11<-c("184646189","184646726")
l2<-c("184646726","184645566")
l3<-c("184645566","184645397")
l4<-c("184645397","184644393")
l5<-c("184646017","184646189","184646726","184645566","184645397","184645397","184644393")
abila1<-abila %>%
filter(TLID %in% l1)
p= npts(abila1, by_feature = TRUE)
abila1 <- cbind(abila1, p) %>%filter(p>1)
abila1<-abila1%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila11<-abila %>%
filter(TLID %in% l11)
p= npts(abila1, by_feature = TRUE)
abila11 <- cbind(abila11, p) %>%filter(p>1)
abila11<-abila11%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila2<-abila %>%
filter(TLID %in% l2)
p= npts(abila2, by_feature = TRUE)
abila2 <- cbind(abila2, p) %>%filter(p>1)
abila2<-abila2%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila3<-abila %>%
filter(TLID %in% l3)
p= npts(abila3, by_feature = TRUE)
abila3 <- cbind(abila3, p) %>%filter(p>1)
abila3<-abila3%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila4<-abila %>%
filter(TLID %in% l4)
p= npts(abila4, by_feature = TRUE)
abila4 <- cbind(abila4, p) %>%filter(p>1)
abila4<-abila4%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila5<-abila %>%
filter(TLID %in% l5)
p= npts(abila5, by_feature = TRUE)
abila5 <- cbind(abila5, p) %>%filter(p>1)
abila5<-abila5%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
data_location <- data %>%
filter(longitude!="",str_detect(message,"hit|van|shoot|driver") ) %>%
add_count(longitude,latitude,author) %>%
rename(Witness=author)
abila5$label=c("van observation","van observation","van observation","fire","car/bike hit","van observation")
ggplot()+
geom_sf(data=abila,size=1,color="grey",fill="cyan1")+
ggtitle("Abila")+
geom_point(data = data_location, aes(x = longitude,y=latitude,color=Witness),size=3)+
geom_point(data = abila5,mapping = aes(x=long,y=lat,label=label),color="red",size=3)+
geom_text(data = abila5,mapping = aes(x=long,y=lat,label=label),color="red",size=3,vjust=2,hjust=-0.1)+
theme(axis.title.y=element_blank(),
axis.ticks.y=element_blank(),
axis.text.y=element_blank(),
axis.title.x=element_blank(),
panel.background = element_rect(fill = "transparent"),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())+
ggtitle("Located Ccdata and Microblogs for Real-time Tracking")+
geom_text()+
geom_line(data = abila1,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
#geom_text(aes(x = 0, y = 0, label = "AAPL"))+
geom_line(data = abila11,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila2,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila3,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila4,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")
data_location$timestamp<-format(data_location$timestamp, format = "%H:%M:%S")
DT::datatable(subset(data_location,select=c(timestamp,Witness,message,longitude,latitude)))
knitr::opts_chunk$set(echo = FALSE)
library(distill)
create_article("Reference")
install.packages("corporaexplorer")
install.packages("devtools")
library("corporaexplorer")
library("devtools")
prepare_data(data$message)
install.packages("devtools")
library("corporaexplorer")
library("devtools")
prepare_data(data$message)
prepare_data(data$cleaned)
explore(data$cleaned)
explore(test)
test<-prepare_data(data$cleaned)
explore(test)
test1<-subset(data,select = c(timestamp,cleaned)
test<-prepare_data(test1)
explore(test1)
test1<-subset(data,select = c(timestamp,cleaned)
test<-prepare_data(test1)
explore(test)
test1<-subset(data,select = c(timestamp,cleaned))
test<-prepare_data(test1)
explore(test)
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning=FALSE)
test1<-subset(data,select = c(timestamp,cleaned))
test<-prepare_data(test1)
