df_ccdata$street1_D<-ifelse(df_ccdata$street1==df_ccdata$street1_D,NA,df_ccdata$street1_D)
df_ccdata$street2<-ifelse(is.na(df_ccdata$street2),df_ccdata$street2_D,df_ccdata$street2)
df_ccdata$street2_D<-ifelse(df_ccdata$street2==df_ccdata$street2_D,NA,df_ccdata$street2_D)
df_ccdata$street1 <- str_replace_all(df_ccdata$street1," [St|Ave]","")
df_ccdata$street1 <- str_replace_all(df_ccdata$street1," ","")
df_ccdata$street2 <- str_replace_all(df_ccdata$street2," [St|Ave]","")
df_ccdata$street2 <- str_replace_all(df_ccdata$street2," ","")
# Read Abila Map
abila<-st_read('data/MC3/Geospatial/Abila.shp',quiet=TRUE)
p = npts(abila, by_feature = TRUE)
abila <- cbind(abila, p) %>%
filter(p>1)
abila<-abila %>% mutate(geometry2=st_touches(geometry))
l1<-c("184646017","184646189")
l11<-c("184646189","184646726")
l2<-c("184646726","184645566")
l3<-c("184645566","184645397")
l4<-c("184645397","184644393")
l5<-c("184646017","184646189","184646726","184645566","184645397","184645397","184644393")
abila1<-abila %>%
filter(TLID %in% l1)
p= npts(abila1, by_feature = TRUE)
abila1 <- cbind(abila1, p) %>%filter(p>1)
abila1<-abila1%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila11<-abila %>%
filter(TLID %in% l11)
p= npts(abila1, by_feature = TRUE)
abila11 <- cbind(abila11, p) %>%filter(p>1)
abila11<-abila11%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila2<-abila %>%
filter(TLID %in% l2)
p= npts(abila2, by_feature = TRUE)
abila2 <- cbind(abila2, p) %>%filter(p>1)
abila2<-abila2%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila3<-abila %>%
filter(TLID %in% l3)
p= npts(abila3, by_feature = TRUE)
abila3 <- cbind(abila3, p) %>%filter(p>1)
abila3<-abila3%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila4<-abila %>%
filter(TLID %in% l4)
p= npts(abila4, by_feature = TRUE)
abila4 <- cbind(abila4, p) %>%filter(p>1)
abila4<-abila4%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
abila5<-abila %>%
filter(TLID %in% l5)
p= npts(abila5, by_feature = TRUE)
abila5 <- cbind(abila5, p) %>%filter(p>1)
abila5<-abila5%>%
mutate(geometry2=st_touches(geometry)) %>%
mutate(long = unlist(map(geometry,2)),
lat = unlist(map(geometry,3)))
data_location <- data %>%
filter(longitude!="",str_detect(message,"hit|van|shoot|driver") ) %>%
add_count(longitude,latitude,author) %>%
rename(Witness=author)
abila5$label=c("van observation","van observation","van observation","fire","car/bike hit","van observation")
ggplot()+
geom_sf(data=abila,size=1,color="grey",fill="cyan1")+
ggtitle("Abila")+
geom_point(data = data_location, aes(x = longitude,y=latitude,color=Witness),size=3)+
geom_point(data = abila5,mapping = aes(x=long,y=lat,label=label),color="red",size=3)+
geom_text(data = abila5,mapping = aes(x=long,y=lat,label=label),color="red",size=3,vjust=2,hjust=-0.1)+
theme(axis.title.y=element_blank(),
axis.ticks.y=element_blank(),
axis.text.y=element_blank(),
axis.title.x=element_blank(),
panel.background = element_rect(fill = "transparent"),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())+
ggtitle("Located Ccdata and Microblogs for Real-time Tracking")+
geom_text()+
geom_line(data = abila1,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
#geom_text(aes(x = 0, y = 0, label = "AAPL"))+
geom_line(data = abila11,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila2,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila3,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")+
geom_line(data = abila4,mapping = aes(x=long,y=lat),color="red",size=1,linetype = "dashed")
#add id to dataset
data$id <- seq.int(nrow(data))
data_subset=subset(data,select=c("time_30min","cleaned"))
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
#add id to dataset
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning=FALSE)
packages= c(
'tidyverse','data.table','lubridate',
'textclean','tm','wordcloud','text2vec',
'topicmodels','tidytext','textmineR','quanteda',
'BTM','textplot','concaveman','ggwordcloud',
'qdapDictionaries','textstem','devtools','textnets',
'igraph', 'tidygraph',
'ggraph', 'visNetwork','udpipe','grid','sp','sf','gridExtra','mapview','tmap')
for(p in packages){
if(!require(p,character.only= T)){
install.packages(p)
}
library(p, character.only = T)
}
#read csv file
data_17_1830=read_csv("data/MC3/csv-1700-1830.csv")
data_1830_20=read_csv("data/MC3/csv-1831-2000.csv")
data_20_2130=read_csv("data/MC3/csv-2001-2131.csv")
#append 3 dataset
data=rbindlist(list(data_17_1830,data_1830_20,data_20_2130))
#print head of data
knitr::kable(head(data,1), "pipe")
#timestamp in lubridate
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_1min = cut(data$timestamp, breaks="1 min")
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))
ppl <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
rt <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
data$cleaned<-data$message %>%
str_replace_all(rt,"")%>%
str_replace_all(ppl,"")%>%
str_replace_all(hash,"") %>%
tolower()%>%   # transform all message to lower cases
replace_contraction()%>%   #replace contractions with long form
replace_word_elongation()%>% #remove the same letter (case insensitive) appears 3 times consecutively
str_replace_all("[0-9]", "") %>% #removing numbers
str_replace_all("([,=!.?$+%-&])","")%>% #remove punctuations
#str_replace_all("rt|pokrally|kronosstar","")%>%
#|#hi|#pok|#pokrally|
# #abilapost|#kronosstar|#centralbulletin|@centralbulletin|@kronosstar|rally|aliba") #remove hashtag and rt
removeWords(stopwords("english"))%>%
str_squish()%>% #trim whitespace from a string
lemmatize_strings()#removes whitespace from start and end of string
# Retrieve RT/ hashtags from message by using regex techniques
regex <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex2 <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex3 <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
regex4 <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
data$RT_pattern<-str_extract_all(data$message, regex, simplify = TRUE)
data$RT_from<-  str_extract_all(data$RT_pattern, regex2, simplify = TRUE)
data$RT_from<-  str_replace(data$RT_from, "@","")
data$RT_message <- str_replace_all(data$message,regex3,"")
data$user_mentioned <- str_extract_all(data$message, regex2, simplify = FALSE)
data$user_mentioned<-data$user_mentioned%>%
str_replace_all("character\\(0\\)|c\\(|\\)|,","")
data_RT<- data %>%
filter(RT_from!="")
knitr::kable(head(subset(data,select=c("message","cleaned"))), "pipe")
GetHashtags <- function(tweet) {
hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>%
as.character()
hashtag.string <- NA
if (length(hashtag.vector) > 0) {
hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
}
return(hashtag.string)
}
hashtags.df <- tibble(
Hashtags = data$message %>% map_chr(.f = ~ GetHashtags(tweet = .x))
)
hashtags.df %>% head()
#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$cleaned)))
#inspect(docs[1:2])
# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')
#convert dataframe to corpus
hashtags.df<-na.omit(hashtags.df)
hashtags.df$id<-seq(nrow(hashtags.df))
hashtags.df$Hashtags<-str_remove(hashtags.df$Hashtags," ")
lst = strsplit(hashtags.df$Hashtags, ",")
names(lst) = hashtags.df$id
hashtags.df2 = melt(lst)
hashtags.df2<-hashtags.df2 %>% group_by(value) %>% count() %>% ungroup()
wordcloud(words = hashtags.df2$value ,freq = hashtags.df2$n, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$user_mentioned)))
#inspect(docs[1:2])
# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')
cc_data<-select(data,"type", "message","timestamp","location")%>%
filter(type=="ccdata")
cc_data2<-cc_data %>%
group_by(message) %>%
count() %>%
arrange(desc(n))
ggplot(cc_data2,aes(x=reorder(message,n), y=n)) +
geom_bar(stat='identity') +
coord_flip()+
ggtitle("Messages from Call Center")+
theme_classic()+
geom_text(aes(label = n),hjust=1,colour = "white")
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning=FALSE)
packages= c(
'tidyverse','data.table','lubridate',
'textclean','tm','wordcloud','text2vec',
'topicmodels','tidytext','textmineR','quanteda',
'BTM','textplot','concaveman','ggwordcloud',
'qdapDictionaries','textstem','devtools','textnets',
'igraph', 'tidygraph',
'ggraph', 'visNetwork','udpipe','grid','sp','sf','gridExtra','mapview','tmap')
for(p in packages){
if(!require(p,character.only= T)){
install.packages(p)
}
library(p, character.only = T)
}
#read csv file
data_17_1830=read_csv("data/MC3/csv-1700-1830.csv")
data_1830_20=read_csv("data/MC3/csv-1831-2000.csv")
data_20_2130=read_csv("data/MC3/csv-2001-2131.csv")
#append 3 dataset
data=rbindlist(list(data_17_1830,data_1830_20,data_20_2130))
#print head of data
knitr::kable(head(data,1), "pipe")
#timestamp in lubridate
data$timestamp <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time_1min = cut(data$timestamp, breaks="1 min")
data$time_30min = cut(data$timestamp, breaks="30 min")
data$id <- seq.int(nrow(data))
ppl <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
rt <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
data$cleaned<-data$message %>%
str_replace_all(rt,"")%>%
str_replace_all(ppl,"")%>%
str_replace_all(hash,"") %>%
tolower()%>%   # transform all message to lower cases
replace_contraction()%>%   #replace contractions with long form
replace_word_elongation()%>% #remove the same letter (case insensitive) appears 3 times consecutively
str_replace_all("[0-9]", "") %>% #removing numbers
str_replace_all("([,=!.?$+%-&])","")%>% #remove punctuations
#str_replace_all("rt|pokrally|kronosstar","")%>%
#|#hi|#pok|#pokrally|
# #abilapost|#kronosstar|#centralbulletin|@centralbulletin|@kronosstar|rally|aliba") #remove hashtag and rt
removeWords(stopwords("english"))%>%
str_squish()%>% #trim whitespace from a string
lemmatize_strings()#removes whitespace from start and end of string
# Retrieve RT/ hashtags from message by using regex techniques
regex <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex2 <- "@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)"
regex3 <- "RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.) "
regex4 <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
hash <- "#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)"
data$RT_pattern<-str_extract_all(data$message, regex, simplify = TRUE)
data$RT_from<-  str_extract_all(data$RT_pattern, regex2, simplify = TRUE)
data$RT_from<-  str_replace(data$RT_from, "@","")
data$RT_message <- str_replace_all(data$message,regex3,"")
data$user_mentioned <- str_extract_all(data$message, regex2, simplify = FALSE)
data$user_mentioned<-data$user_mentioned%>%
str_replace_all("character\\(0\\)|c\\(|\\)|,","")
data_RT<- data %>%
filter(RT_from!="")
knitr::kable(head(subset(data,select=c("message","cleaned"))), "pipe")
GetHashtags <- function(tweet) {
hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>%
as.character()
hashtag.string <- NA
if (length(hashtag.vector) > 0) {
hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
}
return(hashtag.string)
}
hashtags.df <- tibble(
Hashtags = data$message %>% map_chr(.f = ~ GetHashtags(tweet = .x))
)
hashtags.df %>% head()
#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$cleaned)))
#inspect(docs[1:2])
# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')
#convert dataframe to corpus
hashtags.df<-na.omit(hashtags.df)
hashtags.df$id<-seq(nrow(hashtags.df))
hashtags.df$Hashtags<-str_remove(hashtags.df$Hashtags," ")
lst = strsplit(hashtags.df$Hashtags, ",")
names(lst) = hashtags.df$id
hashtags.df2 = melt(lst)
hashtags.df2<-hashtags.df2 %>% group_by(value) %>% count() %>% ungroup()
wordcloud(words = hashtags.df2$value ,freq = hashtags.df2$n, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(data$user_mentioned)))
#inspect(docs[1:2])
# Create a document-term-matrix
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#word cloud
wordcloud(words = df$word, freq = df$freq, min.freq = 5,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
#wordcloud2(data=df, size=1.6, color='random-dark')
cc_data<-select(data,"type", "message","timestamp","location")%>%
filter(type=="ccdata")
cc_data2<-cc_data %>%
group_by(message) %>%
count() %>%
arrange(desc(n))
ggplot(cc_data2,aes(x=reorder(message,n), y=n)) +
geom_bar(stat='identity') +
coord_flip()+
ggtitle("Messages from Call Center")+
theme_classic()+
geom_text(aes(label = n),hjust=1,colour = "white")
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning=FALSE)
#add id to dataset
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
library(distill)
render.site()
render_site()
render_site()
library(rmarkdown)
render_site()
render_site()
View(RT_edges_aggregated_viz)
View(RT_nodes_aggregated)
#add id to dataset
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
#add id to dataset
data_subset=subset(data,select=c("time_30min","cleaned"))
usenet_words<-data_subset%>%
group_by(time_30min) %>%
unnest_tokens(word, cleaned) %>%
count(time_30min,word, sort = TRUE)
#usenet_words[order(usenet_words$time_30min),]
usenet_words$time_30min<-usenet_words$time_30min %>% str_replace_all("2014-01-23 ","")
l1<-c("18:30:00","19:30:00","17:00:00","19:00:00","20:00:00","20:30:00","18:00:00","17:30:00","21:00:00","21:30:00")
Time_30min<-c("18:30-19:00","19:30-20:00","17:00-17:30","19:00-19:30","20:00-20:30","20:30-21:00","18:00-18:30","17:30-18:00","21:00-21:31","21:00-21:31")
time_30min_df<-data.frame(l1,Time_30min)
usenet_words<-left_join(usenet_words,time_30min_df,by=c("time_30min"="l1"))
set.seed(1234)
usenet_words %>%
group_by(Time_30min) %>%
slice_max(order_by = n, n = 20) %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
theme_minimal() +
facet_wrap(~Time_30min)
wordcorpus <- Corpus(VectorSource(as.character(data$cleaned)))
dtm <- DocumentTermMatrix(wordcorpus,
control = list(
wordLengths=c(2, Inf),               # limit word length
bounds = list(global = c(5,Inf)),    # minimum word frequency
removeNumbers = TRUE,                #remove Numbers
weighting = weightTf,                #weighted term frequency
encoding = "UTF-8"))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove 0 dtm rows of matrix
topic=LDA(dtm.new,k=10,method="Gibbs",conrol=list(seed=2021,alpha=0.01,iter=300))
terms(topic,5)
ap_topics <- tidy(topic, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
topic_gamma <- tidy(topic, matrix = "gamma")
topic_gamma <- topic_gamma %>%
group_by(document) %>%
slice(which.max(gamma))
topic_gamma$document<-as.numeric(topic_gamma$document)
#topic_gamma[order(topic_gamma$document),] %>% group_by(topic) %>% count()
#(topic_gamma%>% arrange(desc(-document)))
id_time <- data %>% select(c("id","time_1min"))
topic_data<-left_join(topic_gamma,id_time,by=c("document"="id"))
#manually put topics in LDA results
topic_c<- c(1,2,3,4,5,6,7,8,9,10)
topics_c <- c("police","chatter1","chatter2","fire","chatter3",
"van","chatter4","chatter5","pokrally","pokleader")
topic_df<-data.frame(topic_c,topics_c )
topic_data<-left_join(topic_data,topic_df,by=c("topic"="topic_c"))
topic_data %>% group_by(time_1min,topics_c) %>% count() %>%
ggplot(aes(x=time_1min))+
geom_bar(aes(y=n), stat = "identity",fill = "black")+
facet_wrap(~topics_c)+
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())+
ggtitle("Topics Trend from 1700-2130")
